# High Load System

Ã‡ok fazla kullanÄ±cÄ±dan gelen istekleri hÄ±zlÄ±, gÃ¼venilir ve bozulmadan karÅŸÄ±layabilen sistemlerdir.

1. High Load Sistemlerin Ana Hedefleri

- Scalability (Ã–lÃ§eklenebilirlik): Trafik arttÄ±kÃ§a sistemin Ã§Ã¶kmesi yerine bÃ¼yÃ¼yerek cevap verebilmesidir. (Ã¶rn: Cloud provider)

- Availability (EriÅŸilebilirlik): Sistem Ã§alÄ±ÅŸmaya devam etmeli, kesinti olmamalÄ±.

- Reliability (GÃ¼venilirlik): Veriler kaybolmamalÄ±.

## Realistic Senario:

Diyelim ki kendi URL Shortener sistemimizi yapÄ±yoruz. (bit.ly gibi)

- GÃ¼nlÃ¼k 10 milyar yeni link oluÅŸturuyor.
- GÃ¼nlÃ¼k 2 milyar tÄ±klama geliyor.

Bu ne demek ? 

```
GÃ¼nde 2.000.000.000 tÄ±klama
-> Saniye yaklaÅŸÄ±k 23.148 istek (avarage)
-> Pik zamanlarda bu 5x' e Ã§Ä±kar. ~115.000 istek/saniye
```

ğŸ‘‰ Bu yÃ¼kÃ¼ tek bir sunucu kaldÄ±ramaz. Ä°ÅŸte bu yÃ¼zden High Load mimarisi gerekir.

**High Load Sistemlerin olayÄ± sadece gÃ¼Ã§lÃ¼ donanÄ±m deÄŸil, trafiÄŸi doÄŸru yÃ¶netebilmektir.**

---

# Scaling YÃ¶ntemleri

YÃ¼ksek trafikli sistemler kurgularken ilk bÃ¼yÃ¼k karar ÅŸudur: 

**GÃ¼cÃ¼ artÄ±rarak mÄ± Ã¶lÃ§eceÄŸim, yoksa sayÄ±yÄ± artÄ±rarak mÄ± ?**

1. Vertical Scaling (Scale Up) - "Sunucuyu BÃ¼yÃ¼t"

- Var olan sunucuya daha fazla RAM/CPU/Disk ekliyorsun.
- KolaydÄ±r ama sÄ±nÄ±rÄ± vardÄ±r.

AvantajlarÄ±:
- YÃ¶netmesi kolaydÄ±r. 
- Kodda deÄŸiÅŸiklik gerekmez.

DezavantajlarÄ±: 

- Bir noktadan sonra en bÃ¼yÃ¼k sunucuyu bile satÄ±n alamazsÄ±n.
- Tek bir failure point -> Sunucu Ã§Ã¶kerse tÃ¼m sistem komple gider.

2. Horizontal Scaling (Scale Out) - "Sunucuyu Ã‡oÄŸalt"

- Tek sunucuyu bÃ¼yÃ¼tmek yerine aynÄ± sunucudan 10,100,100 tane koyulmasÄ±.
- Ä°steklerin Load Balancer ile  daÄŸÄ±tÄ±lmasÄ±.

### Load Balancer (YÃ¼k Dengeleyici)

Horizontal Scaling' in kalbi. MÃ¼ÅŸteriden gelen istekleri alÄ±r, sunucularÄ± eÅŸit ÅŸekilde daÄŸÄ±tÄ±r.

```
Client-> Load Balancer -> Server1 / Server2 / Server3

```

**PopÃ¼ler Load Balancer' lar :**

| `YazÄ±lÄ±m BazlÄ±` | `Cloud Managed` |
| --- | --- | 
| nginx,HAProxy |  AWS ELB/ ALB, Google LB, Azure LB  |

> Vertical Scaling neden uzun vadede sÃ¼rdÃ¼rÃ¼lebilir deÄŸildir?

```
Veritical scaling uzun vadeli sÃ¼rdÃ¼rÃ¼lebilir olmamasÄ±nÄ±n sebebi her sunucu bilgisayarÄ±nÄ±n bÃ¼yÃ¼tÃ¼lmesi iÃ§in bir limit olmasÄ±dÄ±r.
```

> Horizontal Scaling kullanÄ±rken tek bir kritik bileÅŸen vardÄ±r â€” o nedir?

```
Horizontal Scaling kullanÄ±rken kritik bileÅŸen Load Balancer' dÄ±r. 
```

> Load Balancer'dan sonra sistemde stateful olmanÄ±n neden sorun olabileceÄŸini tahmin edebiliyor musun?

```
Ã‡Ã¼nkÃ¼ istekler her seferinde farklÄ± sunucuya gidebilir ve sunucu iÃ§inde tutulan session / kullanÄ±cÄ± bilgisi kaybolur.
```

---

# Stateless vs Stateful Sistemler 

Load Balancer ile sunucular Ã§oÄŸaltÄ±ldÄ±ÄŸÄ±nda ÅŸÃ¶yle bir durum oluÅŸur.

```
Client -> Load Balancer -> Server A
Client -> Load Balancer -> Server B
Client -> Load Balancer -> Server C

```

EÄŸer bir kullanÄ±cÄ±yla ilgili oturum (login bilgisi, sepet, geÃ§ici iÅŸlem verisi) sunucunun iÃ§inde saklanmÄ±ÅŸsa buna : 

**Stateful Server** (durum tutan sunucu) denir.

Problem ÅŸudur:

* KullanÄ±cÄ± ilk istekte Server A' ya dÃ¼ÅŸer i login olur (session Server A' da tutulur.)
* Sonraki istek Server B' ye dÃ¼ÅŸerse -> kullanÄ±cÄ± yeniden login olmak zorunda kalÄ±r.

**Ä°ÅŸte bu yÃ¼zden High Load sistemlerde 'State' sunucuda saklanmaz. Ortak bir yere alÄ±nÄ±r.**

Bu sisteme **Stateless Architecture** denir.


> Load Balancer'dan sonra sistemde stateful olmanÄ±n neden sorun olabileceÄŸini tahmin edebiliyor musun?

**Ã‡Ã¶zÃ¼m:**

| `State Saklama Yeri` | `AÃ§Ä±klama`                                                                 |
|  ---                 |  ---                                                                       |
| Redis                | En Ã§ok kullanÄ±lan shared session store                                     |
| Database             | KullanÄ±cÄ± bilgisi DB' de tutulabilir                                       |
| JWT/ Token           | Durumu client tarafÄ±na encode edip geri gÃ¶nderirsin. (sunucu state tutmaz) |

### ğŸ” Bonus: Sticky Session Nedir?

EÄŸer stateless yapÄ± kurmak istemiyorsan Load Balancer' a "Bu kullanÄ±cÄ± hep aynÄ± sunucuya dÃ¼ÅŸsÃ¼n." diyebilirsin. Buna **sticky session/session affinity** denir.

**Ama High Load sistemlerde tavsiye edilmez, Ã§Ã¼nkÃ¼ sunucu Ã¶lÃ¼rse session' da gider.**

Stateful ve Stateless sorununun somut bir Ã¶rneÄŸi

## Ã–rnek E-ticaret Sitesinde Sepet Sistemi:

Sen bir e-ticaret sitesi yapÄ±yorsun (HepsiBurada/ Trendyol gibi). KullanÄ±cÄ± Ã¼rÃ¼ne tÄ±klayÄ±p **sepete ekle** diyor. 

- ğŸ§¨ HatalÄ± Stateful Mimarisi:

```
Client -> Load Balancer -> Server A (Sepeti RAM' de tutuyor.)
```

KullanÄ±cÄ± ikinci Ã¼rÃ¼nÃ¼ eklediÄŸinde: 

```
Client -> Load Balancer -> Server B (Bu sunucuda sepet boÅŸ!)
```

> KullanÄ±cÄ± "Sepetim kayboldu" diye Ã§Ä±ldÄ±rÄ±r. Ã‡Ã¼nkÃ¼ state(durum) sunucuya kilitli kalmÄ±ÅŸtÄ±r.

- âœ… DoÄŸru (Stateless) Mimarisi:

```
Client -> Load Balancer -> Server X -> Sepeti Redis/ DB/ Token' a kayÄ±t eder.
Client -> Load Balancer -> Server Y -> Sepeti Redis' ten okur.
```

**Hangi sunucuya dÃ¼ÅŸerse dÃ¼ÅŸsÃ¼n, client' Ä±n durumu ortak bir yerde tutulur.**

* Soru : Bu Ã¶rnekte State'i merkezi tutmak iÃ§in en mantÄ±klÄ± yer neresi olabilir ? 

1. Database(Sepet tablosu iÃ§ersinde tutmak)
2. Redis(HÄ±zlÄ± cache depolama)
3. JWT Token(Sepeti client tarafÄ±na encode edip cookie iÃ§erisinde taÅŸÄ±mak)

* Cevap : 
```
Ben 2. cevabÄ± seÃ§erdim redis' te tutmak daha mantÄ±klÄ±. Database' de tutmak da mantÄ±klÄ± olabilir.
Ancak tek mantÄ±ksÄ±z olan yol token' da saklamak olduÄŸunu dÃ¼ÅŸÃ¼nÃ¼yorum.
Ã‡Ã¼nkÃ¼ token sÃ¼resinin belirli expire sÃ¼resi var ve bu sÃ¼re belki 1 hafta sonra dolabilir ve sepet datasÄ± kaybolabilir.
```

**Cevap deÄŸerlendirmesi:** 

| `SeÃ§enek`          | `Avantaj`                              | `Dezavantaj`                                                            | `DeÄŸerlendirme`                             |
| ---                | ---                                    | ---                                                                     | ---                                         |
| DB' de saklamak    | KalÄ±cÄ±, gÃ¼venli                        | YavaÅŸ (her sepet aksiyonunda DB sorgusu dÃ¶ner -> yÃ¼k bindirir.)         | MantÄ±klÄ± ama aÄŸÄ±r olabilir.                 |
| Redis' te saklamak | Ã‡ok hÄ±zlÄ±(in-memory) TTL ayarlanabilir.| Redis Ã§Ã¶kse data kaybolabilir -> **ama backup/replication yapÄ±labilir** | Sepet gibi "geÃ§ici state" iÃ§in ideal tercih |
| Token' da saklamak | Sunucusuz, stateless                   | Token ÅŸiÅŸer (bÃ¼yÃ¼k payload), expire olursa sepet kaybolur.              | Riskli                                      |

**Yani Redis, Ã¼Ã§Ã¼nÃ¼n arasÄ±nda en dengeli olanÄ±** -- hem stateless sistemi destekler, hem de yÃ¼ksek trafikte hÄ±zlÄ± Ã§alÄ±ÅŸÄ±r.

---

# Cache & Database Stratejileri (Redis+DB Ä°liÅŸkisi)

Bu derste ÅŸu netleÅŸtirilecek.

> "Veri nereye yazÄ±lmalÄ±? Cache nereye, database nereye? Hangisi Ã¶nce okunmalÄ±?"

**3 Ã§eÅŸit cache stratejisi vardÄ±r.**

1. **Read-Through Cache:** Client DB' ye gitmez. -> Cache' e gider. Cache' de yoksa DB' den Ã§eker. (KullanÄ±cÄ± verileri, profil bilgisi vb.)
2. **Write-Through Cache:** YazÄ±lan her veri Ã¶nce cache' e, sonra DB' ye yazÄ±lÄ±r. (KalÄ±cÄ± olmasÄ± gereken data)
3. **Write-Back (Write-Behind):** Veri Ã¶nce sadece cache'e yazÄ±lÄ±r, sonra arkadan DB' ye flush edilir. (Sepet/ Like sistemi gibi geÃ§ici, toleranslÄ± veriler)

Sistem ve kullanÄ±lan cache stratejisi olarak incelersek: 

| `Sistem` | `Cache Stratejisi`|
| --- | --- | 
| User Profile | **Read-Through** (cache varsa ordan oku, yoksa DB' den Ã§ek.) |
| Sepet | **Write-Back** (Redis'te tut -> Db' ye periyodik yazabilirsin veya hiÃ§ yazmayabilirsin.) |
| Banka Bakiyesi | Asla cache yazarken DB' ye gecikmeli gitmemeli -- **Write-Through** yada direkt DB  |

- Cache, verinin kopyasÄ±dÄ±r. Database' in yerine geÃ§mez.
- Redis gibi cache' ler volatile (uÃ§ucu) olabilir; tamamen kritik olan veriler iÃ§in DB garanti noktasÄ±dÄ±r.
- Sepet gibi geÃ§ici ama hÄ±zlÄ± olmasÄ± gereken ÅŸeyleri Redis' te tutmak en makul yoldur.

**Sepet sipariÅŸ mantÄ±ÄŸÄ±nda 'Sepet = geÃ§ici state', 'SipariÅŸ = kalÄ±cÄ± state' olarak deÄŸerlendirilebilir.**

---

# Replication & Consistency (Master-Slave/ Primary-Replica/ Strong vs Eventual)

Åimdi sistemleri Ã§oÄŸalttÄ±k (scaling), cache' e aldÄ±k (Redis). SÄ±radaki soru: 

> Database' i nasÄ±l Ã¶lÃ§eklendiririz?
> Yani High Load altÄ±nda okumalar ve yazmalar nasÄ±l daÄŸÄ±tÄ±lÄ±r? 

## Database Replication Nedir ? 

Bir DB' yi birden fazla kopya olarak tutmak demektir.

**Roller:**
1. Primary(Master): TÃ¼m yazmalar (INSERT/UPDATE) buraya yapÄ±lÄ±r.
2. Replica(Slave/Read Replica): Sadece okumalar yapÄ±lÄ±r.(SELECT) 

Yazma/ Okuma AyrÄ±mÄ± BÃ¶yle Ã‡alÄ±ÅŸÄ±r: 

```
Client(Write) -> Primary DB
Client(Read)  -> Replica DB' ler
```

BÃ¶ylece yÃ¼k paylaÅŸÄ±lmÄ±ÅŸ olur.

âš ï¸ Ancak burada bir sorun var : **Replication Lag**

- Primary DB' ye veri yazÄ±lÄ±r.
- Replica bu veriyi hemen mi alÄ±r? -> *HayÄ±r, Ã§oÄŸu zaman gecikmeli gelir.*

Bu yÃ¼zden ortaya iki tip tutarlÄ±lÄ±k modeli Ã§Ä±kar:

1. **Strong Consistency:** YazÄ±lan veri anÄ±nda tÃ¼m DB' lerde gÃ¶rÃ¼nÃ¼r. (Ã–rn: BankacÄ±lÄ±k Sistemleri) 
2. **Eventual Consistency:** Veriler kÄ±sa sÃ¼reliÄŸine farklÄ± olabilir, ama sonunda eÅŸitlenir. (Ã–rn: Sosyal Medya like sayÄ±larÄ±)

### Eventual Consistency' nin Klasik Ã–rnekleri:

Sen instagramda bir postu like' ladÄ±n.

- Hemen altta "12502 -> 12503 beÄŸeni" olur.
- ArkadaÅŸÄ±n telefonunda hala 12502 gÃ¶rÃ¼nÃ¼yor.

**Bu bir bug deÄŸil eventual consistency' dir.**

* Soru: Bir e-ticaret sitesinde "Ã¼rÃ¼n stok sayÄ±sÄ±" sence hangi modelle tutulmalÄ±? (Strong mu, Eventual mÄ±?) Neden ? 

* Cevap: Bir e-ticaret sitesinde Ã¼rÃ¼nÃ¼n stok sayÄ±sÄ± strong consistency ile tutulabilir. 
          Ã‡Ã¼nkÃ¼ sepete ekleyip sipariÅŸ verildiÄŸinde eÄŸer bir gecikme yaÅŸanÄ±rsa sonrasÄ±nda sipariÅŸin iptali gibi
          mÃ¼ÅŸterilerde olumsuz durumlara yol aÃ§ma riski vardÄ±r.

* Soru: **KullanÄ±cÄ± profiline ait gÃ¶sterilen takipÃ§i sayÄ±sÄ±** sence strong consistency mi gerektirir yoksa eventual olabilir mi?

* Cevap: KullanÄ±cÄ± profiline ait gÃ¶sterilen takipÃ§i sayÄ±sÄ± eventual consistency ile iÅŸlem yapÄ±lmasÄ± daha saÄŸlÄ±klÄ± olur. 
         Ã‡Ã¼nkÃ¼ 1. cevapdaki gibi Ã§ok Ã§ok olumsuz durumlara ve sonrasÄ±nda yaÅŸanacak krizlere sebebiyet verecek bir Ã¶ncelik olmadÄ±ÄŸÄ±nÄ± dÃ¼ÅŸÃ¼nÃ¼yorum.

**Kritik olmayan datalarda Eventual Consistency, kritik datalarda Strong Consistency seÃ§imi yapÄ±lÄ±r. High Load sistem tasarÄ±mÄ± tam da bu dengeyi kurmak demektir.**

---

# Partitioning/ Sharding (GerÃ§ek Ã–lÃ§eklenme Burada BaÅŸlar.)

Cache ekledik. Replication yaptÄ±k ama bir noktada tek bir Primary DB bile yetmeyecek.

Ä°ÅŸte bu durumda yapÄ±lacak ÅŸey: 

> Database'i bÃ¶lmek.

## Partitioning/ Sharding Nedir? 

**Devasa bir tabloyu veya database' iki ya da daha fazla parÃ§aya bÃ¶lmek.**

BÃ¶ylece:

```
instread of -> 1 DB with 1M users
we do -> 10 DB each with 10K users
```

### Sharding YÃ¶ntemleri:

1. **Range Sharding:** Alfabetik veya ID aralÄ±ÄŸÄ±na gÃ¶re *(Ã–rn: A-M -> DB1, N-Z -> DB2)*
2. **Hash Sharding:** ID' yi hash' leyip mod alarak *(Ã–rn: user_id % 4 -> 4 farklÄ± shard)*
3. **Geo Sharding:** Lokasyona gÃ¶re *(Ã–rn: Avrupa ayrÄ± DB, Amerika ayrÄ± DB)*
4. **Feature/Entity Based:** Veri tipine gÃ¶re *(Ã–rn: dbo.Users tablosu farklÄ±, dbo.Orders farklÄ± DB' de )*

* Ã–rnek: KullanÄ±cÄ±larÄ± Hash ile Shard Et:

```
Shard = user_id % 4 

user_id = 341 => 341 % 4 = 1 -> Database A 
user_id = 762 => 762 % 4 = 2 -> Database B

```

**Her sunucu sadece kendi parÃ§asÄ±nÄ± bilir -> yÃ¼k daÄŸÄ±tÄ±lÄ±r,sorgular hÄ±zlanÄ±r.**

**Sharding SorunlarÄ±:**

1. **Cross-Shard Query:** "Bir kullanÄ±cÄ±nÄ±n 5 sipariÅŸi, 3 yorumu ve 2 adresi var." -> hepsi farklÄ± DB' de ise toplamak zorlaÅŸÄ±r.
2. **Shard Rebalancing:** "Shard 1 Ã§ok doldu,2 boÅŸ kaldÄ±" gibi durumlarda yeniden daÄŸÄ±tÄ±m gerektirir.
3. **ID Uniqueness:** FarklÄ± shard' lardaki veriler Ã§akÄ±ÅŸabilir. -> *global_id* sistemi gerekebilir.

* Soru: Sence â€œKullanÄ±cÄ± tablosuâ€nu sharding yapmak mantÄ±klÄ±dÄ±r ama â€œÃœrÃ¼n kategorileriâ€ tablosunu sharding yapmak mantÄ±klÄ± mÄ±dÄ±r? Neden?

* Cevap: Bence Ã¼rÃ¼n kategorileri tablosunu sharding yapmak Ã§ok deÄŸildir. Ã‡Ã¼nkÃ¼ kategorilerin mutlak bir sÄ±nÄ±rÄ± vardÄ±r. Bu yÃ¼zden datanÄ±n bÃ¶lÃ¼nmesi gerektiÄŸini dÃ¼ÅŸÃ¼nmÃ¼yorum.

* Soru: Sharding yapÄ±ldÄ±ÄŸÄ±nda "user_id = 1234" iÃ§in hangi shard kullanÄ±labileceÄŸini nasÄ±l bilebiliriz. (Senin Ã¶nerdiÄŸin yÃ¶ntem)

* Cevap: Ã‡ok bÃ¼yÃ¼k bir trafik dÃ¼ÅŸÃ¼ndÃ¼ÄŸÃ¼mÃ¼zde mesela amazon gibi. Ã–nce Geo Sharding yÃ¶ntemi ile bÃ¶lgelere ayÄ±rÄ±p ardÄ±ndan aynÄ± bÃ¶lgelerdeki kullanÄ±cÄ±lar iÃ§in de Hash Sharding kullanarak daha verimli bir bÃ¶lÃ¼mleme ile sistemin daha stabil ve hÄ±zlÄ± Ã§alÄ±ÅŸabileceÄŸini dÃ¼ÅŸÃ¼nmÃ¼yorum.
Mesela TÃ¼rkiye' de kayÄ±t olan bir amazon kullanÄ±cÄ±sÄ± Geo Sharding yÃ¶ntemi ile TR sunucularÄ±na kayÄ±t olacak ardÄ±ndan TR' de bulunan istanbul, Ankara veya Ä°zmir gibi illerde bulunan sunucularÄ±nda Hash Sharding kullanarak veri paylaÅŸÄ±mÄ± yapmak daha saÄŸlÄ±klÄ± olabilir. 

Cevap Amerika iÃ§in yine Amazon gibi yÃ¼ksek trafikli bir sistemden bahsedecek olursak ilk olarak Geo sharing ile kÄ±ta bazlÄ± ABD sunucusu olarak bÃ¶ler sonrasÄ±nda yine Geo sharding ile bu sefer ABD iÃ§erisindeki sunucularÄ± eyalet bazlÄ± bÃ¶lerdim ve eyalatlerde bulunan sunucularÄ± Hash Sharding ile bÃ¶lmeyi dÃ¼ÅŸnÃ¼rdÃ¼m. Mesela Washington iÃ§in aktif 30 sunucu varsa bu 30 sunucu da Hash Sharding yÃ¶ntemi uygulardÄ±m. 

> **EÄŸer data Ã§ok kÃ¼Ã§Ã¼kse -> bÃ¶lmek yerine tÃ¼m sunuculara kopyalamak daha mantÄ±klÄ±dÄ±r. (Replication)**
> **EÄŸer data bÃ¼yÃ¼kse -> parÃ§alamak zorundayÄ±z. (Sharding)**






