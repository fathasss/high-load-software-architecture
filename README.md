# High Load System

Ã‡ok fazla kullanÄ±cÄ±dan gelen istekleri hÄ±zlÄ±, gÃ¼venilir ve bozulmadan karÅŸÄ±layabilen sistemlerdir.

1. High Load Sistemlerin Ana Hedefleri

- Scalability (Ã–lÃ§eklenebilirlik): Trafik arttÄ±kÃ§a sistemin Ã§Ã¶kmesi yerine bÃ¼yÃ¼yerek cevap verebilmesidir. (Ã¶rn: Cloud provider)

- Availability (EriÅŸilebilirlik): Sistem Ã§alÄ±ÅŸmaya devam etmeli, kesinti olmamalÄ±.

- Reliability (GÃ¼venilirlik): Veriler kaybolmamalÄ±.

## Realistic Senario:

Diyelim ki kendi URL Shortener sistemimizi yapÄ±yoruz. (bit.ly gibi)

- GÃ¼nlÃ¼k 10 milyar yeni link oluÅŸturuyor.
- GÃ¼nlÃ¼k 2 milyar tÄ±klama geliyor.

Bu ne demek ?

```
GÃ¼nde 2.000.000.000 tÄ±klama
-> Saniye yaklaÅŸÄ±k 23.148 istek (avarage)
-> Pik zamanlarda bu 5x' e Ã§Ä±kar. ~115.000 istek/saniye
```

ğŸ‘‰ Bu yÃ¼kÃ¼ tek bir sunucu kaldÄ±ramaz. Ä°ÅŸte bu yÃ¼zden High Load mimarisi gerekir.

**High Load Sistemlerin olayÄ± sadece gÃ¼Ã§lÃ¼ donanÄ±m deÄŸil, trafiÄŸi doÄŸru yÃ¶netebilmektir.**

---

# Scaling YÃ¶ntemleri

YÃ¼ksek trafikli sistemler kurgularken ilk bÃ¼yÃ¼k karar ÅŸudur:

**GÃ¼cÃ¼ artÄ±rarak mÄ± Ã¶lÃ§eceÄŸim, yoksa sayÄ±yÄ± artÄ±rarak mÄ± ?**

1. Vertical Scaling (Scale Up) - "Sunucuyu BÃ¼yÃ¼t"

- Var olan sunucuya daha fazla RAM/CPU/Disk ekliyorsun.
- KolaydÄ±r ama sÄ±nÄ±rÄ± vardÄ±r.

AvantajlarÄ±:

- YÃ¶netmesi kolaydÄ±r.
- Kodda deÄŸiÅŸiklik gerekmez.

DezavantajlarÄ±:

- Bir noktadan sonra en bÃ¼yÃ¼k sunucuyu bile satÄ±n alamazsÄ±n.
- Tek bir failure point -> Sunucu Ã§Ã¶kerse tÃ¼m sistem komple gider.

2. Horizontal Scaling (Scale Out) - "Sunucuyu Ã‡oÄŸalt"

- Tek sunucuyu bÃ¼yÃ¼tmek yerine aynÄ± sunucudan 10,100,100 tane koyulmasÄ±.
- Ä°steklerin Load Balancer ile daÄŸÄ±tÄ±lmasÄ±.

### Load Balancer (YÃ¼k Dengeleyici)

Horizontal Scaling' in kalbi. MÃ¼ÅŸteriden gelen istekleri alÄ±r, sunucularÄ± eÅŸit ÅŸekilde daÄŸÄ±tÄ±r.

```
Client-> Load Balancer -> Server1 / Server2 / Server3

```

**PopÃ¼ler Load Balancer' lar :**

| `YazÄ±lÄ±m BazlÄ±` | `Cloud Managed`                   |
| --------------- | --------------------------------- |
| nginx,HAProxy   | AWS ELB/ ALB, Google LB, Azure LB |

> Vertical Scaling neden uzun vadede sÃ¼rdÃ¼rÃ¼lebilir deÄŸildir?

```
Veritical scaling uzun vadeli sÃ¼rdÃ¼rÃ¼lebilir olmamasÄ±nÄ±n sebebi her sunucu bilgisayarÄ±nÄ±n bÃ¼yÃ¼tÃ¼lmesi iÃ§in bir limit olmasÄ±dÄ±r.
```

> Horizontal Scaling kullanÄ±rken tek bir kritik bileÅŸen vardÄ±r â€” o nedir?

```
Horizontal Scaling kullanÄ±rken kritik bileÅŸen Load Balancer' dÄ±r.
```

> Load Balancer'dan sonra sistemde stateful olmanÄ±n neden sorun olabileceÄŸini tahmin edebiliyor musun?

```
Ã‡Ã¼nkÃ¼ istekler her seferinde farklÄ± sunucuya gidebilir ve sunucu iÃ§inde tutulan session / kullanÄ±cÄ± bilgisi kaybolur.
```

---

# Stateless vs Stateful Sistemler

Load Balancer ile sunucular Ã§oÄŸaltÄ±ldÄ±ÄŸÄ±nda ÅŸÃ¶yle bir durum oluÅŸur.

```
Client -> Load Balancer -> Server A
Client -> Load Balancer -> Server B
Client -> Load Balancer -> Server C

```

EÄŸer bir kullanÄ±cÄ±yla ilgili oturum (login bilgisi, sepet, geÃ§ici iÅŸlem verisi) sunucunun iÃ§inde saklanmÄ±ÅŸsa buna :

**Stateful Server** (durum tutan sunucu) denir.

Problem ÅŸudur:

- KullanÄ±cÄ± ilk istekte Server A' ya dÃ¼ÅŸer i login olur (session Server A' da tutulur.)
- Sonraki istek Server B' ye dÃ¼ÅŸerse -> kullanÄ±cÄ± yeniden login olmak zorunda kalÄ±r.

**Ä°ÅŸte bu yÃ¼zden High Load sistemlerde 'State' sunucuda saklanmaz. Ortak bir yere alÄ±nÄ±r.**

Bu sisteme **Stateless Architecture** denir.

> Load Balancer'dan sonra sistemde stateful olmanÄ±n neden sorun olabileceÄŸini tahmin edebiliyor musun?

**Ã‡Ã¶zÃ¼m:**

| `State Saklama Yeri` | `AÃ§Ä±klama`                                                                 |
| -------------------- | -------------------------------------------------------------------------- |
| Redis                | En Ã§ok kullanÄ±lan shared session store                                     |
| Database             | KullanÄ±cÄ± bilgisi DB' de tutulabilir                                       |
| JWT/ Token           | Durumu client tarafÄ±na encode edip geri gÃ¶nderirsin. (sunucu state tutmaz) |

### ğŸ” Bonus: Sticky Session Nedir?

EÄŸer stateless yapÄ± kurmak istemiyorsan Load Balancer' a "Bu kullanÄ±cÄ± hep aynÄ± sunucuya dÃ¼ÅŸsÃ¼n." diyebilirsin. Buna **sticky session/session affinity** denir.

**Ama High Load sistemlerde tavsiye edilmez, Ã§Ã¼nkÃ¼ sunucu Ã¶lÃ¼rse session' da gider.**

Stateful ve Stateless sorununun somut bir Ã¶rneÄŸi

## Ã–rnek E-ticaret Sitesinde Sepet Sistemi:

Sen bir e-ticaret sitesi yapÄ±yorsun (HepsiBurada/ Trendyol gibi). KullanÄ±cÄ± Ã¼rÃ¼ne tÄ±klayÄ±p **sepete ekle** diyor.

- ğŸ§¨ HatalÄ± Stateful Mimarisi:

```
Client -> Load Balancer -> Server A (Sepeti RAM' de tutuyor.)
```

KullanÄ±cÄ± ikinci Ã¼rÃ¼nÃ¼ eklediÄŸinde:

```
Client -> Load Balancer -> Server B (Bu sunucuda sepet boÅŸ!)
```

> KullanÄ±cÄ± "Sepetim kayboldu" diye Ã§Ä±ldÄ±rÄ±r. Ã‡Ã¼nkÃ¼ state(durum) sunucuya kilitli kalmÄ±ÅŸtÄ±r.

- âœ… DoÄŸru (Stateless) Mimarisi:

```
Client -> Load Balancer -> Server X -> Sepeti Redis/ DB/ Token' a kayÄ±t eder.
Client -> Load Balancer -> Server Y -> Sepeti Redis' ten okur.
```

**Hangi sunucuya dÃ¼ÅŸerse dÃ¼ÅŸsÃ¼n, client' Ä±n durumu ortak bir yerde tutulur.**

- 1ï¸âƒ£ : Bu Ã¶rnekte State'i merkezi tutmak iÃ§in en mantÄ±klÄ± yer neresi olabilir ?

1. Database(Sepet tablosu iÃ§ersinde tutmak)
2. Redis(HÄ±zlÄ± cache depolama)
3. JWT Token(Sepeti client tarafÄ±na encode edip cookie iÃ§erisinde taÅŸÄ±mak)

- Cevap :

```
Ben 2. cevabÄ± seÃ§erdim redis' te tutmak daha mantÄ±klÄ±. Database' de tutmak da mantÄ±klÄ± olabilir.
Ancak tek mantÄ±ksÄ±z olan yol token' da saklamak olduÄŸunu dÃ¼ÅŸÃ¼nÃ¼yorum.
Ã‡Ã¼nkÃ¼ token sÃ¼resinin belirli expire sÃ¼resi var ve bu sÃ¼re belki 1 hafta sonra dolabilir ve sepet datasÄ± kaybolabilir.
```

**Cevap deÄŸerlendirmesi:**

| `SeÃ§enek`          | `Avantaj`                               | `Dezavantaj`                                                            | `DeÄŸerlendirme`                             |
| ------------------ | --------------------------------------- | ----------------------------------------------------------------------- | ------------------------------------------- |
| DB' de saklamak    | KalÄ±cÄ±, gÃ¼venli                         | YavaÅŸ (her sepet aksiyonunda DB sorgusu dÃ¶ner -> yÃ¼k bindirir.)         | MantÄ±klÄ± ama aÄŸÄ±r olabilir.                 |
| Redis' te saklamak | Ã‡ok hÄ±zlÄ±(in-memory) TTL ayarlanabilir. | Redis Ã§Ã¶kse data kaybolabilir -> **ama backup/replication yapÄ±labilir** | Sepet gibi "geÃ§ici state" iÃ§in ideal tercih |
| Token' da saklamak | Sunucusuz, stateless                    | Token ÅŸiÅŸer (bÃ¼yÃ¼k payload), expire olursa sepet kaybolur.              | Riskli                                      |

**Yani Redis, Ã¼Ã§Ã¼nÃ¼n arasÄ±nda en dengeli olanÄ±** -- hem stateless sistemi destekler, hem de yÃ¼ksek trafikte hÄ±zlÄ± Ã§alÄ±ÅŸÄ±r.

---

# Cache & Database Stratejileri (Redis+DB Ä°liÅŸkisi)

Bu derste ÅŸu netleÅŸtirilecek.

> "Veri nereye yazÄ±lmalÄ±? Cache nereye, database nereye? Hangisi Ã¶nce okunmalÄ±?"

**3 Ã§eÅŸit cache stratejisi vardÄ±r.**

1. **Read-Through Cache:** Client DB' ye gitmez. -> Cache' e gider. Cache' de yoksa DB' den Ã§eker. (KullanÄ±cÄ± verileri, profil bilgisi vb.)
2. **Write-Through Cache:** YazÄ±lan her veri Ã¶nce cache' e, sonra DB' ye yazÄ±lÄ±r. (KalÄ±cÄ± olmasÄ± gereken data)
3. **Write-Back (Write-Behind):** Veri Ã¶nce sadece cache'e yazÄ±lÄ±r, sonra arkadan DB' ye flush edilir. (Sepet/ Like sistemi gibi geÃ§ici, toleranslÄ± veriler)

Sistem ve kullanÄ±lan cache stratejisi olarak incelersek:

| `Sistem`       | `Cache Stratejisi`                                                                       |
| -------------- | ---------------------------------------------------------------------------------------- |
| User Profile   | **Read-Through** (cache varsa ordan oku, yoksa DB' den Ã§ek.)                             |
| Sepet          | **Write-Back** (Redis'te tut -> Db' ye periyodik yazabilirsin veya hiÃ§ yazmayabilirsin.) |
| Banka Bakiyesi | Asla cache yazarken DB' ye gecikmeli gitmemeli -- **Write-Through** yada direkt DB       |

- Cache, verinin kopyasÄ±dÄ±r. Database' in yerine geÃ§mez.
- Redis gibi cache' ler volatile (uÃ§ucu) olabilir; tamamen kritik olan veriler iÃ§in DB garanti noktasÄ±dÄ±r.
- Sepet gibi geÃ§ici ama hÄ±zlÄ± olmasÄ± gereken ÅŸeyleri Redis' te tutmak en makul yoldur.

**Sepet sipariÅŸ mantÄ±ÄŸÄ±nda 'Sepet = geÃ§ici state', 'SipariÅŸ = kalÄ±cÄ± state' olarak deÄŸerlendirilebilir.**

---

# Replication & Consistency (Master-Slave/ Primary-Replica/ Strong vs Eventual)

Åimdi sistemleri Ã§oÄŸalttÄ±k (scaling), cache' e aldÄ±k (Redis). SÄ±radaki soru:

> Database' i nasÄ±l Ã¶lÃ§eklendiririz?
> Yani High Load altÄ±nda okumalar ve yazmalar nasÄ±l daÄŸÄ±tÄ±lÄ±r?

## Database Replication Nedir ?

Bir DB' yi birden fazla kopya olarak tutmak demektir.

**Roller:**

1. Primary(Master): TÃ¼m yazmalar (INSERT/UPDATE) buraya yapÄ±lÄ±r.
2. Replica(Slave/Read Replica): Sadece okumalar yapÄ±lÄ±r.(SELECT)

Yazma/ Okuma AyrÄ±mÄ± BÃ¶yle Ã‡alÄ±ÅŸÄ±r:

```
Client(Write) -> Primary DB
Client(Read)  -> Replica DB' ler
```

BÃ¶ylece yÃ¼k paylaÅŸÄ±lmÄ±ÅŸ olur.

âš ï¸ Ancak burada bir sorun var : **Replication Lag**

- Primary DB' ye veri yazÄ±lÄ±r.
- Replica bu veriyi hemen mi alÄ±r? -> _HayÄ±r, Ã§oÄŸu zaman gecikmeli gelir._

Bu yÃ¼zden ortaya iki tip tutarlÄ±lÄ±k modeli Ã§Ä±kar:

1. **Strong Consistency:** YazÄ±lan veri anÄ±nda tÃ¼m DB' lerde gÃ¶rÃ¼nÃ¼r. (Ã–rn: BankacÄ±lÄ±k Sistemleri)
2. **Eventual Consistency:** Veriler kÄ±sa sÃ¼reliÄŸine farklÄ± olabilir, ama sonunda eÅŸitlenir. (Ã–rn: Sosyal Medya like sayÄ±larÄ±)

### Eventual Consistency' nin Klasik Ã–rnekleri:

Sen instagramda bir postu like' ladÄ±n.

- Hemen altta "12502 -> 12503 beÄŸeni" olur.
- ArkadaÅŸÄ±n telefonunda hala 12502 gÃ¶rÃ¼nÃ¼yor.

**Bu bir bug deÄŸil eventual consistency' dir.**

- 1ï¸âƒ£: Bir e-ticaret sitesinde "Ã¼rÃ¼n stok sayÄ±sÄ±" sence hangi modelle tutulmalÄ±? (Strong mu, Eventual mÄ±?) Neden ?

- Cevap: Bir e-ticaret sitesinde Ã¼rÃ¼nÃ¼n stok sayÄ±sÄ± strong consistency ile tutulabilir.
  Ã‡Ã¼nkÃ¼ sepete ekleyip sipariÅŸ verildiÄŸinde eÄŸer bir gecikme yaÅŸanÄ±rsa sonrasÄ±nda sipariÅŸin iptali gibi
  mÃ¼ÅŸterilerde olumsuz durumlara yol aÃ§ma riski vardÄ±r.

- 2ï¸âƒ£: **KullanÄ±cÄ± profiline ait gÃ¶sterilen takipÃ§i sayÄ±sÄ±** sence strong consistency mi gerektirir yoksa eventual olabilir mi?

- Cevap: KullanÄ±cÄ± profiline ait gÃ¶sterilen takipÃ§i sayÄ±sÄ± eventual consistency ile iÅŸlem yapÄ±lmasÄ± daha saÄŸlÄ±klÄ± olur.
  Ã‡Ã¼nkÃ¼ 1. cevapdaki gibi Ã§ok Ã§ok olumsuz durumlara ve sonrasÄ±nda yaÅŸanacak krizlere sebebiyet verecek bir Ã¶ncelik olmadÄ±ÄŸÄ±nÄ± dÃ¼ÅŸÃ¼nÃ¼yorum.

**Kritik olmayan datalarda Eventual Consistency, kritik datalarda Strong Consistency seÃ§imi yapÄ±lÄ±r. High Load sistem tasarÄ±mÄ± tam da bu dengeyi kurmak demektir.**

---

# Partitioning/ Sharding (GerÃ§ek Ã–lÃ§eklenme Burada BaÅŸlar.)

Cache ekledik. Replication yaptÄ±k ama bir noktada tek bir Primary DB bile yetmeyecek.

Ä°ÅŸte bu durumda yapÄ±lacak ÅŸey:

> Database'i bÃ¶lmek.

## Partitioning/ Sharding Nedir?

**Devasa bir tabloyu veya database' iki ya da daha fazla parÃ§aya bÃ¶lmek.**

BÃ¶ylece:

```
instread of -> 1 DB with 1M users
we do -> 10 DB each with 10K users
```

### Sharding YÃ¶ntemleri:

1. **Range Sharding:** Alfabetik veya ID aralÄ±ÄŸÄ±na gÃ¶re _(Ã–rn: A-M -> DB1, N-Z -> DB2)_
2. **Hash Sharding:** ID' yi hash' leyip mod alarak _(Ã–rn: user_id % 4 -> 4 farklÄ± shard)_
3. **Geo Sharding:** Lokasyona gÃ¶re _(Ã–rn: Avrupa ayrÄ± DB, Amerika ayrÄ± DB)_
4. **Feature/Entity Based:** Veri tipine gÃ¶re _(Ã–rn: dbo.Users tablosu farklÄ±, dbo.Orders farklÄ± DB' de )_

- Ã–rnek: KullanÄ±cÄ±larÄ± Hash ile Shard Et:

```
Shard = user_id % 4

user_id = 341 => 341 % 4 = 1 -> Database A
user_id = 762 => 762 % 4 = 2 -> Database B

```

**Her sunucu sadece kendi parÃ§asÄ±nÄ± bilir -> yÃ¼k daÄŸÄ±tÄ±lÄ±r,sorgular hÄ±zlanÄ±r.**

**Sharding SorunlarÄ±:**

1. **Cross-Shard Query:** "Bir kullanÄ±cÄ±nÄ±n 5 sipariÅŸi, 3 yorumu ve 2 adresi var." -> hepsi farklÄ± DB' de ise toplamak zorlaÅŸÄ±r.
2. **Shard Rebalancing:** "Shard 1 Ã§ok doldu,2 boÅŸ kaldÄ±" gibi durumlarda yeniden daÄŸÄ±tÄ±m gerektirir.
3. **ID Uniqueness:** FarklÄ± shard' lardaki veriler Ã§akÄ±ÅŸabilir. -> _global_id_ sistemi gerekebilir.

- 1ï¸âƒ£: Sence â€œKullanÄ±cÄ± tablosuâ€nu sharding yapmak mantÄ±klÄ±dÄ±r ama â€œÃœrÃ¼n kategorileriâ€ tablosunu sharding yapmak mantÄ±klÄ± mÄ±dÄ±r? Neden?

- Cevap: Bence Ã¼rÃ¼n kategorileri tablosunu sharding yapmak Ã§ok deÄŸildir. Ã‡Ã¼nkÃ¼ kategorilerin mutlak bir sÄ±nÄ±rÄ± vardÄ±r. Bu yÃ¼zden datanÄ±n bÃ¶lÃ¼nmesi gerektiÄŸini dÃ¼ÅŸÃ¼nmÃ¼yorum.

- 2ï¸âƒ£: Sharding yapÄ±ldÄ±ÄŸÄ±nda "user_id = 1234" iÃ§in hangi shard kullanÄ±labileceÄŸini nasÄ±l bilebiliriz. (Senin Ã¶nerdiÄŸin yÃ¶ntem)

- Cevap: Ã‡ok bÃ¼yÃ¼k bir trafik dÃ¼ÅŸÃ¼ndÃ¼ÄŸÃ¼mÃ¼zde mesela amazon gibi. Ã–nce Geo Sharding yÃ¶ntemi ile bÃ¶lgelere ayÄ±rÄ±p ardÄ±ndan aynÄ± bÃ¶lgelerdeki kullanÄ±cÄ±lar iÃ§in de Hash Sharding kullanarak daha verimli bir bÃ¶lÃ¼mleme ile sistemin daha stabil ve hÄ±zlÄ± Ã§alÄ±ÅŸabileceÄŸini dÃ¼ÅŸÃ¼nÃ¼yorum.
  Mesela TÃ¼rkiye' de kayÄ±t olan bir amazon kullanÄ±cÄ±sÄ± Geo Sharding yÃ¶ntemi ile TR sunucularÄ±na kayÄ±t olacak ardÄ±ndan TR' de bulunan istanbul, Ankara veya Ä°zmir gibi illerde bulunan sunucularÄ±nda Hash Sharding kullanarak veri paylaÅŸÄ±mÄ± yapmak daha saÄŸlÄ±klÄ± olabilir. Cevap Amerika iÃ§in yine Amazon gibi yÃ¼ksek trafikli bir sistemden bahsedecek olursak ilk olarak Geo sharing ile kÄ±ta bazlÄ± ABD sunucusu olarak bÃ¶ler sonrasÄ±nda yine Geo sharding ile bu sefer ABD iÃ§erisindeki sunucularÄ± eyalet bazlÄ± bÃ¶lerdim ve eyalatlerde bulunan sunucularÄ± Hash Sharding ile bÃ¶lmeyi dÃ¼ÅŸnÃ¼rdÃ¼m. Mesela Washington iÃ§in aktif 30 sunucu varsa bu 30 sunucu da Hash Sharding yÃ¶ntemi uygulardÄ±m.

> **EÄŸer data Ã§ok kÃ¼Ã§Ã¼kse -> bÃ¶lmek yerine tÃ¼m sunuculara kopyalamak daha mantÄ±klÄ±dÄ±r. (Replication)**

> **EÄŸer data bÃ¼yÃ¼kse -> parÃ§alamak zorundayÄ±z. (Sharding)**

---

# Message Queues & Asenkron Ä°ÅŸlem (Event Driven Architecture)

## âœ… Message Queue Nedir?

> Ãœretilen olaraylarÄ± (event) _hemen iÅŸlemek sÄ±raya koymak_ ve asenkron **(gecikmeli ama garantili)** ÅŸekilde iÅŸlemek iÃ§in kullanÄ±lan sistemdir.

### ğŸ“ŒNeden KullanÄ±lÄ±r?

Ã‡Ã¼nkÃ¼ gerÃ§ek dÃ¼nyada bazÄ± iÅŸlemler hemen yapÄ±lmak zorunda deÄŸildir.

| `Ä°ÅŸlem`                                        | `Senkron Durumu`                              |
| ---------------------------------------------- | --------------------------------------------- |
| KullanÄ±cÄ± 'Pay' butonuna bastÄ± - > Ã–deme OnayÄ± | Senkron olmalÄ±(Hemen cevap vermeli)           |
| Ã–demeden sonra 'Mail GÃ¶nder'                   | Asenkron (SÄ±ra kuyruÄŸa yaz, arkadan iÅŸlenir.) |
| Like sayÄ±sÄ±nÄ± artÄ±r                            | Asenkron yapÄ±labilir                          |
| KullanÄ±cÄ±ya bildirim gÃ¶nder                    | KuyruÄŸa atÄ±labilir                            |
| bÃ¼yÃ¼k resmi sÄ±kÄ±ÅŸtÄ±r.                          | KuyruÄŸa at                                    |

### Queue Kullanmazsan Ne Olur ?

Ã–rnek:

```
POST /buy

-> DB insert
-> Stok azalt
-> Mail gÃ¶nder
-> PDF Fatura oluÅŸtur
-> Bildirim gÃ¶nder
-> Slack mesajÄ± gÃ¶nder
```

Hepsini aynÄ± anda yaparsan 5 saniye sÃ¼rer. KullanÄ±cÄ± "bozuldu mu?" diye sayfayÄ± kapatÄ±r.

### Queue ile DoÄŸru Mimari

```
POST /buy
-> DB insert + Stok dÃ¼ÅŸ (sadece ana iÅŸlem)
-> Event "OrderCreated" kuyruÄŸa yazÄ±lÄ±r
-> Worker' lar arkadan: Mail, Fatura, Bildirim, Slack gÃ¶nderir.
```

KullanÄ±cÄ±ya cevap verilir. -> **SipariÅŸ AlÄ±ndÄ±**

Arka planda herÅŸey devam eder.

**Message Queue Ã–rnekleri:**

| `Sistem`             | `Type`            | `KullanÄ±m`                      |
| -------------------- | ----------------- | ------------------------------- |
| RabbitMQ             | Message Broker    | KÃ¼Ã§Ã¼k-Orta Sistemler            |
| Kafka                | Event Streaming   | YÃ¼ksek throughput -> 1M+ events |
| AWS SQS, GCP Pub/Sub | Cloud Queue       | Serverless kullanÄ±m             |
| Redis Stream         | Lightweight Queue | Basit Ä°ÅŸler iÃ§in                |

- 1ï¸âƒ£: Bir e-ticaret sitesinde "SipariÅŸ verildiÄŸinde mail gÃ¶nderme" iÅŸlemi sence Senkron mu olmalÄ±, Asenkron mu? Neden ?

- Cevap: Bence asenkron olmalÄ±. Senkron bir sipariÅŸ akÄ±ÅŸÄ± tasarlanmÄ±ÅŸ ve canlÄ± data yazÄ±lmÄ±ÅŸ. KullanÄ±cÄ± artÄ±k sipariÅŸini vermiÅŸ ve gerekli stok iÅŸlemleri yapÄ±lmÄ±ÅŸ. Bu yÃ¼zden mail kullanÄ±cÄ± iÃ§in bir ek bilgilendirme. KuyruÄŸa alÄ±nÄ±p bekletilebilir.

- 2ï¸âƒ£: Like butonuna basÄ±ldÄ±ÄŸÄ±nda sayacÄ± hemen artÄ±rmak mÄ± gerekir, yoksa Queue' ya atÄ±p gecikmeli artÄ±rmak da olur mu?

- Cevap: Ä°lk sayaÃ§ iÅŸlemi cache' de yapÄ±lÄ±p cache' deki sayacÄ±n artÄ±lÄ±rÄ±lÄ±p canlÄ± yazma iÅŸlemi asenkron yapÄ±lmasÄ± daha saÄŸlÄ±klÄ± olur. Ã‡Ã¼nkÃ¼ Ã¼nlÃ¼ bir influencer' Ä±n sosyal medya postu saniyeler iÃ§erisinde Ã§ok fazla trafik alabilir. Bu yÃ¼zden canlÄ± data olarak aktarma iÅŸlemi asenkron olarak yapÄ±lmasÄ± ve kuyruk yapÄ±sÄ± ile kurgulanmasÄ± daha saÄŸlÄ±klÄ± olur.

**Ä°kinci sorunun cevabÄ± iÃ§in "Burst Traffic" ihtimalinin dÃ¼ÅŸÃ¼nÃ¼lmÃ¼ÅŸ olmasÄ± gayet mantÄ±klÄ±. BÃ¼yÃ¼k sistemler (Instagram, Twitter) exactly this! yÃ¶ntemini kullanmaktadÄ±r.**

---

# Eventual Consistency GerÃ§ek DÃ¼nyada NasÄ±l YÃ¶netilir?

Consistency konusuna Ã¶nce teorik olarak baktÄ±k ama ÅŸimdi eventual consistency ile yapÄ±lan tutarsÄ±zlÄ±klar nasÄ±l yÃ¶netilir kÄ±smÄ±na geÃ§elim.

## Senaryo:

Bir e-ticaret sitesindesin.

- KullanÄ±cÄ± SipariÅŸi Verdi -> Ã–deme AlÄ±ndÄ± âœ…
- Ama arkaplanda Ã§alÄ±ÅŸan asenkron iÅŸlemlerden biri **BAÅARISIZ** oldu. (Ã¶rneÄŸin stok gÃ¼ncellemesi yapmadÄ±.)

| `Strateji`                         | `AÃ§Ä±klama`                                                                                 | `Risk`                                                  |
| ---------------------------------- | ------------------------------------------------------------------------------------------ | ------------------------------------------------------- |
| Strong Consistency (Transactional) | TÃ¼m adÄ±mlar aynÄ± anda yapÄ±lÄ±r, biri fail olursa geri alÄ±nÄ±r.                               | KullanÄ±cÄ± bekler, yavaÅŸlatÄ±r.                           |
| Eventual Consistency + Retry Queue | BaÅŸaramayan event tekrar kuyruÄŸa atÄ±lÄ±r ve yeniden denenir.                                | BazÄ± event' lar _eninde sonunda_ olur ama geÃ§ olabilir. |
| SAGA Pattern (Compensating Action) | **EÄŸer stok gÃ¼ncellemesi baÅŸarÄ±sÄ±z ise -> SipariÅŸi otomatik iptal et ve parayÄ± geri Ã¶de.** | KullanÄ±cÄ±yÄ± Ã¼zmeden rollback.                           |

## SAGA Pattern (DaÄŸÄ±tÄ±k TransactionlarÄ±n KurtarÄ±cÄ±sÄ±)

| `Step` | `Process`           | `Is Success?`              |
| ------ | ------------------- | -------------------------- |
| 1      | Payment-> Success   | Compensate: Para iadesi    |
| 2      | Stok dÃ¼ÅŸ            | Compensate: Stok Geri Ekle |
| 3      | SipariÅŸi DB' ye yaz | Compensate: SatÄ±r sil      |
| 4      | Mail gÃ¶nder         | (Gerek yok, sadece logla)  |

- 1ï¸âƒ£ Eventual consistency kullanÄ±lan sistemlerde â€œbaÅŸarÄ±sÄ±z eventâ€™lerâ€ iÃ§in nasÄ±l bir strateji uygulanmalÄ±dÄ±r? Retry mi, kompanzasyon mu, loglayÄ±p bÄ±rakmak mÄ±? Sen nasÄ±l karar verirsin?

- Cevap: BaÅŸarÄ±sÄ±z event' lar iÃ§in transactional iÅŸlem uygulanabilir. Bir sipariÅŸin verildiÄŸini varsayalÄ±m. Ã–deme ve Stoktan dÃ¼ÅŸme iÅŸlemi Strong Consistency yapÄ±sÄ± ile yapÄ±lmÄ±ÅŸ olsun.
  Kuyruk yapÄ±sÄ± kullanÄ±larak mail gÃ¶nderme gibi iÅŸlemler sÃ¶z konusu. Kuyrukda hata alÄ±ndÄ±ÄŸÄ±nda mail gÃ¶nderme, fatura kesme gibi iÅŸlemler SAGA Pattern ile yapÄ±labilir.
  Yada yalnÄ±zca Ã¶deme adÄ±mlarÄ± Strong Consistency ile yapÄ±ldÄ±ysa. Stoktan dÃ¼ÅŸme, mail gÃ¶nderme ve fatura gÃ¶nderme gibi iÅŸlemlerde SAGA Pattern kullanÄ±larak iÅŸlemler geri alÄ±nabilir sipariÅŸ iptal edilip para iadesi saÄŸlanabilir. Ä°ki yolda saÄŸlÄ±klÄ±dÄ±r.

- 2ï¸âƒ£ Sence bir bankacÄ±lÄ±k sisteminde â€œpara transferiâ€ eventual consistency olabilir mi? Yoksa strong mu olmak zorunda?

- Cevap: Bence bir bankacÄ±lÄ±k sisteminde para transferi iÅŸlemi kuyruk yapÄ±sÄ± ile yapÄ±lmalÄ±. Yani Eventrual Consistency ve denenebilir kuyruk yapÄ±sÄ± kullanÄ±labilir. Ã‡Ã¼nkÃ¼ bankacÄ±lÄ±k sistemlerinde para transferi yapÄ±lÄ±rken istek yapÄ±lan servislerde farklÄ± farklÄ± sorgulama servisleri Ã§alÄ±ÅŸabilir ve bu trafiÄŸin yÃ¶netilmesi iÃ§in Eventual Consistency daha uygundur.

---

# Read-Heavy Trafikte Replication & CQRS

### Problem:

Bir sistemde:

1. %10 Write (INSERT /UPDATE/ DELETE)
2. %90 Read (SELECT)

yapÄ±lÄ±yor olsun.

Bu durumda bÃ¼tÃ¼n read isteklerini primary database' den yapmak mantÄ±klÄ± mÄ±dÄ±r ?

> HayÄ±r. Ã‡Ã¼nkÃ¼ primary sÃ¼rekli yazma da yapÄ±yor. -> Okuma yÃ¼kÃ¼ geldiÄŸinde yavaÅŸlar.

### Ã‡Ã¶zÃ¼m:

| `Veri`                        | `Nereden okunur?`  | `Nereden yazÄ±lÄ±r?` |
| ----------------------------- | ------------------ | ------------------ |
| Yazma iÅŸlemi (CREATE/ UPDATE) | -> Primary DB      | âœ…                 |
| Okuma iÅŸlemi (GET/ LIST)      | -> Read Replica DB | âœ…                 |

Bu yapÄ±ya Ã§oÄŸu zaman CQRS (Command Query Responsibility Segregation) denir.

**Mimari:**

```
        WRITE
App -------------> Primary DB
|
READ
|
-----------------> Read-Replica-1
|
-----------------> Read-Replica-2
```

- 1ï¸âƒ£ Sadece **read heavy** olan bir endpoint (Ã¶rneÄŸin: Trend Ã¼rÃ¼nleri listeleme) hangi DBâ€™den okunmalÄ±? Primary mÄ±, Replica mÄ±?

- Cevap: Trend Ã¼rÃ¼nleri listelemek farklÄ± replica database'lerden okunmasÄ± daha saÄŸlÄ±klÄ± olur. Ã‡Ã¼nkÃ¼ trend Ã¼rÃ¼nler belirli ortalamaya gÃ¶re deÄŸiÅŸiklik gÃ¶sterebilir. DeÄŸiÅŸen bir veri olduÄŸu iÃ§in Replica' dan okumak daha saÄŸlÄ±klÄ± olabilir.

- 2ï¸âƒ£ Replicaâ€™dan okuduÄŸumuz iÃ§in â€œanlÄ±k deÄŸiÅŸiklikleri 1-2 saniyelik gecikmeyle gÃ¶rÃ¼rsekâ€ bu problem olur mu? Hangi senaryoda sorun olur, hangi senaryoda olmaz?

- Cevap: Sorun olabilecek senaryolardan bir tanesi anlÄ±k Ã§ok hÄ±zlÄ± deÄŸiÅŸen veriler. Mesela like sayÄ±larÄ± ya da Ã¼nlÃ¼ influencer' Ä±n yaptÄ±ÄŸÄ± paylaÅŸÄ±mÄ±n altÄ±nda gelen yorumlar veya reddit gibi platformlarda aÃ§Ä±lmÄ±ÅŸ trend konularda etkileÅŸimlerin yavaÅŸlamasÄ±nÄ± saÄŸlayabilir. Bu noktada Redis gibi bir yapÄ±nÄ±n kullanÄ±lmasÄ± daha saÄŸlÄ±klÄ± olur.

```
Primary DB(Write) -> DoÄŸru ve GÃ¼Ã¼ncel Veri -> Ani yÃ¼kte zorlanÄ±r -> Ã–deme,Stok dÃ¼ÅŸme
Read Replica -> Read' i Ã¶lÃ§eklendirir -> 1-3 saniye gecikmeli olabilir -> Profil gÃ¶rÃ¼ntÃ¼leme, Trend Ã¼rÃ¼n listesi
Redis Cache -> Ultra HÄ±zlÄ± Okuma -> Veri eski olabilir -> Like sayÄ±sÄ±, Hot Content
```

> HerÅŸey DB' den **okunmaz**, her veri **aynÄ± ciddiyette deÄŸildir.**

---

# Rate Limiting & Throttling (KullanÄ±cÄ±larÄ± ve BotlarÄ± Kontrol Etme)

> Bir kullanÄ±cÄ± (veya bot) saniyede 1000 istek atarsa ne yaparsÄ±n ?

1. **Rate Limit:** Belirli bir sÃ¼re iÃ§inde bir kaynaÄŸa kaÃ§ kez eriÅŸilebileceÄŸini kontrol eder. (1 IP -> dakikada en fazla 30 Login denemesi)
2. **Throttling:** Limit aÅŸÄ±lÄ±rsa istekleri tamamen engellemek yerine yavaÅŸlatÄ±r. (Videoyu 1080P deÄŸil 480P vermek)
3. **Quota:** KullanÄ±cÄ±ya uzun dÃ¶nemli bir limit verir. (Free User -> Ayda 5000 API Ã§aÄŸrÄ±sÄ±)

## Rate Limiti NasÄ±l UygularÄ±z ?

Rate limit uygulanabilecek 4 farklÄ± katman bulunur.

1. **Client Side (UI):** "Bu butona hÄ±zlÄ± basmayÄ±n" uyarÄ±sÄ± -> ZayÄ±f ama iyi UX
2. **Backend Gateway/ API Gateway:** NGINX, Envoy, Kong, AWS API Gateway -> En sÄ±k kullanÄ±lan katman
3. **Middleware/ Code Level:** Express/ Nest/ Spring interceptor -> Esnek ama app yÃ¼klenir
4. **Firewall/ Network Level:** Cloudflare, AWS WAF -> Botlara karÅŸÄ± en etkili

> **GerÃ§ek dÃ¼nya sistemlerinde Rate limit genelde Redis + Sliding Window/ Token Bucket algoritmasÄ±yla yapÄ±lÄ±r.**

- 1ï¸âƒ£ Bir login endpointâ€™i iÃ§in Rate Limiting uygulanmalÄ± mÄ±? NasÄ±l bir limit koyardÄ±n?

- Cevap: Evet. Login endpoint' i iÃ§in bir Rate Limit kullanÄ±rdÄ±m. Ã‡Ã¼nkÃ¼ kaba kuvvet saldÄ±rÄ±sÄ±na maruz kalabilir. Bunun iÃ§inde saniyede login endpoint'i iÃ§in bir limiti olurdu ve bu limit aÅŸÄ±ldÄ±ÄŸÄ±nda gelen isteÄŸin IP' sini kara listeye alÄ±rdÄ±m.

- 2ï¸âƒ£ APIâ€™yi kullanan premium ve free kullanÄ±cÄ±larÄ± farklÄ± ÅŸekilde sÄ±nÄ±rlamak ister misin? NasÄ±l?

- Cevap: Evet sÄ±nÄ±rlamak isterim. API' nin Premium sÃ¼rÃ¼mÃ¼ iÃ§in ayda 100.000 API Ã§aÄŸrÄ±sÄ± alabilecek ÅŸekilde sÄ±nÄ±rlandÄ±rÄ±rÄ±m. AyrÄ±ca Free sÃ¼rÃ¼mÃ¼ iÃ§inde aylÄ±k 10.000 istek ile kÄ±sÄ±tlardÄ±m.

---

# Token Bucket AlgoritmasÄ±

Rate limiting sistemlerde en Ã§ok kullanÄ±lan algoritma:

> Token Bucket (ve varyasyonu: Leaky Bucket)

## Token Bucket MantÄ±ÄŸÄ±

Her kullanÄ±cÄ±ya bir bucket (kova) veriyoruz.

| `Kova Ã¶zelliÄŸi`            | `AnlamÄ±`                                                                                      |
| -------------------------- | --------------------------------------------------------------------------------------------- |
| Kova kapasitesi (capacity) | Maksimum kaÃ§ istek hakkÄ± birikebilir ?                                                        |
| Token refill rate          | Her saniye ne kadar yeni hak veriyoruz ?                                                      |
| Request GeldiÄŸinde         | Kovada token varsa -> Ä°zin verilir ve 1 token dÃ¼ÅŸÃ¼rÃ¼lÃ¼r. Yoksa -> Reddedilir veya bekletilir. |

- Ã–rnek (Free KullanÄ±cÄ±)

| `Ã–zellik`   | `DeÄŸer`                                  |
| ----------- | ---------------------------------------- |
| Capacity    | 100 Token                                |
| Refill Rate | 1 token/ saniye (yani dakikada 60 istek) |

- KullanÄ±cÄ± bir anda 100 istek atabilir. (kovada doluysa)
- Ama sÃ¼rekli atmak isterse en fazla 60 istek/ dakika yapabilir.

- Ã–rnek (Premium KullanÄ±cÄ±)

| `Ã–zellik`   | `Free`    | `Premium`  |
| ----------- | --------- | ---------- |
| Capacity    | 100 Token | 1000 token |
| Refill Rate | 1 t/s     | 10 t/s     |

- ğŸ‘‰ Login endpointâ€™i iÃ§in hangi rate limiting modelini seÃ§erdin?

1. Sliding Window (Zaman aralÄ±ÄŸÄ±nda sabit limit â€” Ã¶rn: 5 dakika iÃ§inde en fazla 10 login denemesi)
2. Token Bucket (Saniyede 1 token â€” anlÄ±k patlamaya izin verir ama sÃ¼rdÃ¼rÃ¼lebilir deÄŸilse keser)
3. Fixed Window (Her 1 dakikada bir limit sÄ±fÄ±rlanÄ±r)

- Cevap: EÄŸer yalnÄ±zca bir tanesini kullanacak isem Sliding Window rate limit modelini seÃ§erdim. Ã‡Ã¼nkÃ¼ brute force karÅŸÄ± daha Ã¶nemli bir yÃ¶ntem olurdu. Ama eÄŸer birden fazla kullanacaksam Sliding Window ve Token Bucket yÃ¶ntemlerini bir arada kullanabilirdim. Login endpoint' i iÃ§in belirli sayÄ±da bir token olurdu mesela 3 token. Ä°lk 3 denemede parolayÄ± hatÄ±rlamadÄ±ysa kullanÄ±cÄ± Token Bucket' ta login endpoint iÃ§in olan istekleri sÄ±fÄ±rlardÄ±m ve iki adÄ±mlÄ± doÄŸrulama kullanmasÄ±nÄ± isterdim. Burda da aÅŸÄ±rÄ± istek oluyorsa ÅŸayet sliding window ile belirlediÄŸim limite gÃ¶re baÄŸlantÄ±yÄ± kapatÄ±rdÄ±m.

---

# Circuit & Breaker Failover

- Neden Gerekli ?

Bir servise ya da dÄ±ÅŸa baÄŸÄ±mlÄ± bir kaynaÄŸa sÃ¼rekli baÅŸarÄ±sÄ±z istek atmak tÃ¼m sistemi yavaÅŸlatÄ±r veya Ã§Ã¶kertir. **Circuit Breaker** bu tÃ¼r aksaklÄ±klarda aracÄ± olarak davranÄ±r: belirli bir baÅŸarÄ±sÄ±zlÄ±k eÅŸiÄŸine gelince daha fazla istek gÃ¶ndermeyi keser, sistemin geri kalanÄ±nÄ± korur. Failover ise arÄ±zalÄ± servisten trafiÄŸi baÅŸka saÄŸlÄ±klÄ± servise yÃ¶nlendirmektir.

## Temel Kavramlar

1. **Timeouts:** Her isteÄŸin makul bir sÃ¼re iÃ§inde geri dÃ¶nmediÄŸni dÃ¼ÅŸÃ¼nerek zaman aÅŸÄ±mÄ± koyar.
2. **Retries with Backoff:** Hata alÄ±ndÄ±ÄŸÄ±nda hemen tekrar denemek yerine artan gecikme (exponential backoff + jitter) uygula.
3. **Circuit Breaker States:**

- Closed : Normal durum istekler gÃ¶nderilir.
- Open: Hatalar yÃ¼ksek -> istekler reddedilir kÄ±sa sÃ¼reliÄŸine.
- Half-Open: Belirli aralÄ±k sonra az sayÄ±da deneme gÃ¶nderilir; baÅŸarÄ±lÄ±ysa Closed' a dÃ¶ner, baÅŸarÄ±sÄ±zsa tekrar Open.
- Bulkhead: KaynaklarÄ± bÃ¶lÃ¼mlere ayÄ±r; bir bÃ¶lÃ¼m Ã§Ã¶kse diÄŸerleri Ã§alÄ±ÅŸmaya devam eder. (Ã¶rn: thread pool' lar).
- Failover/ Graceful Degradation: Bir hizmet Ã§Ã¶ktÃ¼ÄŸÃ¼nde alternatif servis, cached response veya degraded ux sun.

### Tipik Circuit Breaker KurallarÄ± (pratik)

- windowSize: son N istekte ne kadar hata yoksa?
- failureThreshold: % hata oranÄ± (Ã¶rn. %50) veya hatalÄ± istek sayÄ±sÄ± (Ã¶rn. 20) aÅŸÄ±lÄ±rsa -> Open
- openTimeout: Open durumunda bekleme sÃ¼resi (Ã¶rn. 30s).
- halfOpenMaxCalls: Half-open' dayken kaÃ§ istek test edilecek. (Ã¶rn. 5).
- successThreshold: Half-open testlerde kaÃ§ baÅŸarÄ±lÄ± istek ÅŸart (Ã¶rn. 3) -> Closed.

**Ã–rnek: PseudoCode (basit circuit breaker)**

```pseudo

class CircuitBreaker{
        state = "CLOSED"
        failures = 0
        successCount = 0
        lastFailure = null

        onRequest(call):
            if state == "OPEN":
                if now-lastFailureTime < OPEN_TIMEOUT:
                    throw CircuitOpenException
                else:
                    state = "HALF_OPEN"
                    successCount = 0

            try:
                resp = call()
                onSuccess()
                return resp
            except Exception:
                onFailure()
                throw

        onSuccess():
            if state == "HALF_OPEN":
                successCount += 1
                if successCount >= SUCCESS_THRESHOLD:
                    state = "CLOSED"
                    failures = 0
                else:
                    failures = 0

        onFailures():
            failures += 1
            lastFailuretime = now
            if  failures > FAILURE_THRESHOLD:
                state = "OPEN"
}

```

### Retries & Backoff (iyi uygulama)

1. **MaxRetries:** 3
2. **Backoff:** exponential (e.g., 100ms, 200ms, 400ms)
3. **Jitter:** kÃ¼Ã§Ã¼k rastgeleleÅŸtirme ekle, thundering herd Ã¶nlenir.
4. **Idempotency:** retry edilebilecek Ã§aÄŸrÄ±lar idempotent olmalÄ± veya idempotency-key kullanÄ±lmalÄ±. (Ã¶rn: Ã¶deme iÅŸlemleri iÃ§in)

### Failover & Routing

1. **Active-Passive:** Primary' den hata alÄ±ndÄ±ÄŸÄ±nda Secondary' ye yÃ¶nlendir.
2. **Active-Active:** Trafik her iki bÃ¶lgeye de gider; bir bÃ¶lge dÃ¼ÅŸerse diÄŸerleri kalÄ±r.
3. **Geo-Failover:** BÃ¶lge dÃ¼ÅŸerse DNS veya global Load Balancer ile trafiÄŸi baÅŸka bÃ¶lgeye taÅŸÄ±.

Ã–rnek : DB read-replicas -> read from nearest replica; primary fail olursa promote replica veya route clients to other region.

### Bulkhead & Resource Isolation

- Her dÄ±ÅŸ API iÃ§in ayrÄ± thread pool veya connection pool kullan.
- bir API yoÄŸunlaÅŸÄ±rsa diÄŸer API' nin kaynaklarÄ±nÄ± yemez.

### Graceful Degradation (KullanÄ±cÄ±ya nasÄ±l davranÄ±rÄ±z?)

EÄŸer bir microservis dÃ¶nmÃ¼yorsa:

- Serve cached data (Ã¶nbellekleki son iyi sonuÃ§)
- Minimal/ degraded UI (Ã¶rn. "ÅŸu an yorumlar yÃ¼kleniyor")
- Fail-fast: uzun bekletme yerine hÄ±zlÄ± fallback dÃ¶ndÃ¼rÃ¼r.

### Monitoring & Metrics (mutlaka izle)

Her circuit breaker iÃ§in:

- requestRate
- errorRate
- latency (P50/ P95/ P99)
- state changes (Open->Half Open-> Closed)
- retry count
- downstream latency

UyarÄ±lar(alert):

- Circuit Breaker open sayÄ±sÄ± artarsa (yÃ¼zde veya absolut)
- Latency P95 > hedef
- Retries > threshold

**GerÃ§ek Hayatta KullanÄ±m Ã–rnekleri:**

- Netflix Hystrix(eski) -> circuit breaker implementasyonu Ã¼nlÃ¼dÃ¼r.
- Envoy ve Istio gibi servis mesh' ler circuit breaker, retries ve timeouts saÄŸlar.
- API Gateway (NGINX, Kong, AWS API GW)

### Ã–rnek Senaryo: (Ã–deme Servisi)

1. KullanÄ±cÄ± Ã¶deme yapar. -> call PaymentService()
2. EÄŸer PaymentService timeout veya hata verirse:

- Retry(Maks 3) exponential backoff ile.
- EÄŸer CB aÃ§Ä±ldÄ±ysa: fallback-> "Ã–deme iÅŸleminiz ÅŸuan yavaÅŸ. LÃ¼tfen faha sonra tekrar deneyin veya destek ile iltiÅŸime geÃ§in."
- Background: enqueue event for manual investigation or retry.

AyrÄ±ca Ã¶deme iÃ§in idempotency-key kullanÄ±lmalÄ±, tekrar Ã§aÄŸrÄ±da Ã§ifte Ã¶deme engellenmeli.

**TERÄ°MLER**

| `Terim`             | `AÃ§Ä±klama`                                                                                  | `Ã–rnek`                                                                         |
| ------------------- | ------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------- |
| Latency             | Gecikme sÃ¼resi(bir isteÄŸin gidiÅŸ-geliÅŸ zamanÄ±)                                              | "Whatsapp mesajÄ± ne kadar sÃ¼rede karÅŸÄ±ya gidiyor?"                              |
| Backoff             | Hata olduÄŸunda tekrar denemeden Ã¶nce bekleme sÃ¼resi                                         | KapÄ± kilitli -> 1 saniye sonra tekrar dene, yine kilitli -> 2 saniye sonra dene |
| Exponential Backoff | Bekleme sÃ¼resi katlanarak artÄ±yorsa                                                         | 1s-> 2s-> 4s-> 8s ÅŸeklinde beklemek                                             |
| Jitter              | Backoff sÃ¼resine rastgelelilik eklemek                                                      | 4 saniye beklemesi gerekirken bazen 3.8s veya 4.2 saniye bekletmek              |
| Thundering Herd     | BirÃ§ok sistem aynÄ± anda saldÄ±rÄ±ya geÃ§erse oluÅŸturduÄŸu aÅŸÄ±rÄ± yÃ¼k (sÃ¼rÃ¼ psikolojisi problemi) | 100 telefon aynÄ± anda Amazon'a "Åimdi Yenile" dediÄŸinde sistem dÃ¼ÅŸer            |
| Enqueue             | KuyruÄŸa eklemek                                                                             | "Bir iÅŸi sonra yap" diye bir listeye yazmak                                     |

- 1ï¸âƒ£ Circuit breaker hangi Ã¼Ã§ durumda Open olabilir?

* PeÅŸ peÅŸe Ã§ok fazla hata olursa (Ã¶rneÄŸin 10 denemenin 7â€™si hata)
* Timeout sayÄ±sÄ± artarsa (servis cevap vermiyor)
* BaÄŸlantÄ± kurulamazsa (DNS / Network arÄ±zasÄ±)

- 2ï¸âƒ£ Retry stratejisinde jitter neden Ã¶nemlidir?

Ã‡Ã¼nkÃ¼ tÃ¼m sistemler aynÄ± anda tekrar denerse bÃ¼yÃ¼k Ã§Ã¶kÃ¼ÅŸ olur. Jitter kÃ¼Ã§Ã¼k rastgele bekleme koyarak "sÃ¼rÃ¼ halinde saldÄ±rÄ±yÄ±" engeller.

- 3ï¸âƒ£ Bulkhead nedir?

KaynaklarÄ± bÃ¶lmeye denir. Ã–rneÄŸin "Ã¶deme servisine ayrÄ±lmÄ±ÅŸ 20 thread, diÄŸer API' lere 20 thread" diye bÃ¶lmek. BÃ¶ylece Ã¶deme servisi yavaÅŸlasa bile diÄŸer servisleri bozmaz.

**Circuit Breaker + Retry + Failover mimarisinin sade ve anlaÅŸÄ±lÄ±r bir Mermaid diyagramÄ±:**

```mermaid

flowchart TD

Client -->|Request| API_Gateway

API_Gateway -->|Calls| Circuit_Breaker

Circuit_Breaker -->|Service Available| Service_A
Circuit_Breaker -->|Service Down| Retry_Logic

Retry_Logic -->|Max Retries Reached| Failover_Service
Retry_Logic -->|Retry Success| Service_A

Service_A -->|Response| Circuit_Breaker
Failover_Service -->|Fallback Response| Circuit_Breaker

Circuit_Breaker -->|Returns| API_Gateway
API_Gateway -->|Final Response| Client

```

**Basit Okuma MantÄ±ÄŸÄ±:**

| `BileÅŸen`        | `AÃ§Ä±klama`                                                      |
| ---------------- | --------------------------------------------------------------- |
| API Gateway      | TÃ¼m isteklerin giriÅŸ noktasÄ±                                    |
| Circuit Breaker  | HatalarÄ± takip eder -> Gerekirse servisi "bloklar"              |
| Retry Logic      | Yeniden deneme (backoff + jitter)                               |
| Failover Service | Ana servis Ã§Ã¶kerse devreye giren yedek                          |
| Final Response   | MÃ¼ÅŸteriye "ya gerÃ§ek cevap ya da gracefull fallback" gÃ¶sterilir |

---

# Service Discovery & API Gateway TasarÄ±mÄ±

Bu dersle birlikte artÄ±k microservis mimarisinin network katmanÄ±na giriÅŸ yapÄ±yoruz.

Bundan Ã¶nce ÅŸunu Ã§Ã¶zdÃ¼k:

- Bir sistem nasÄ±l Ã¶lÃ§eklenir ?
- Trafik Ã§ok geldiÄŸinde nasÄ±l korunur ?
- Servis hata verirse nasÄ±l ayaÄŸa kalkar ?

Åimdi sorumuz ÅŸu:

> Bir sÃ¼rÃ¼ mikroservis var. Bunlar birbirlerini nasÄ±l buluyor? Trafik nasÄ±l doÄŸru servise yÃ¶nlendiriyor?

## Temel Kavramlar

| `Kavram`          | `AÃ§Ä±klama`                                                         | `Analojisi`                                                          |
| ----------------- | ------------------------------------------------------------------ | -------------------------------------------------------------------- |
| Service Discovery | Servislerin birbirini otomatik olarak bulmasÄ±nÄ± saÄŸlayan mekanizma | "ArkadaÅŸlarÄ±nÄ±n telefon numarasÄ±nÄ± ezberlemek yerine rehbere bakmak" |
| API Gateway       | DÄ±ÅŸ dÃ¼nyadan gelen tÃ¼m isteklerin tek giriÅŸ noktasÄ±                | ApartmanÄ±n kapÄ±cÄ±sÄ± gibi - gelen herkesi ilgili kata yÃ¶nlendirir     |
| Service Registry  | Servislerin adreslerinin (IP/ Port) kayÄ±tlÄ± olduÄŸu yer             | Telefon rehberi                                                      |
| Ingress/ Egress   | Sisteme giriÅŸ/ sistemden dÄ±ÅŸarÄ± Ã§Ä±kan trafik                       | KapÄ± giriÅŸ-Ã§Ä±kÄ±ÅŸ kontrolÃ¼                                            |
| Sidecar Pattern   | Her servisin yanÄ±nda bir "ajan" container                          | "Koruma gÃ¶revlisiyle dolaÅŸan Ã¼nlÃ¼"                                   |

### Service Discovery TÃ¼rleri

1. **Client-side Discovery:** Ä°stemci(client) direkt registry' den sorar "hangi servis nerede?" (Ã¶rn: Netflix OSS (Eureka + Ribbon))
2. **Server-side Discovery:** API Gateway/ Load Balancer yÃ¶nlendirir. (Kubernates + Envoy/ Istio combo)

**Ã–rnek Senaryo:**

- SipariÅŸ Servisi -> Ã–deme Servisi' ne istek atacak ama hangi IP' de Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± bilmiyor. Ã‡Ã¼nkÃ¼ container her yeniden deploy edildiÄŸinde IP deÄŸiÅŸiyor.

**Ã‡Ã¶zÃ¼m:**

1. Payment Service ayaÄŸa kalktÄ±ÄŸÄ±nda Service Registry' ye kaydolur.
2. Order Service registry' den "payment nerede?" diye sorar.
3. Ä°stek doÄŸru yere gider.

```mermaid
flowchart LR

Client --> API_Gateway

API_Gateway --> Service_Registry

API_Gateway -->|Route Request| Order_Service

Order_Service --> Service_Registry
Order_Service -->|Call Payment| Payment_Service

```

**Soru:** Sence neden "her servis birbirinin IP'sini ezberlemek yerine Service Registry kullanmalÄ±" ?

**Ã‡Ã¶zÃ¼m:** Ã‡Ã¼nkÃ¼ mikroservis ortamÄ± **dinamik**tir. IP'ler deploy' da deÄŸiÅŸir, servis sayÄ±sÄ± sÃ¼rekli artar/azalÄ±r, auto-scaling devreye girer. BÃ¶yle bir ortamda servisler birbirinin IP' sini bilseydi her gÃ¼n kaos Ã§Ä±kardÄ±. O yÃ¼zden herkes "kimin nerede olduÄŸunu" Service Registry' den sorar.

- AynÄ± bir Google Rehberi gibi(Contacts) dÃ¼ÅŸÃ¼n.
- Her arkadaÅŸÄ±nÄ±n numarasÄ±nÄ± ezberlemiyorsun, sadece "adÄ±"' yla buluyorsun.Mikroservislerde de aynÄ± mantÄ±k.

# API Gateway'ler ve Trafik YÃ¶netimi

> DÄ±ÅŸ dÃ¼nyadan gelen bÃ¼tÃ¼n isteklerin geÃ§tiÄŸi tek kapÄ±dÄ±r.
> Her istek buradan geÃ§er, kimlik doÄŸrulama, rate limit, loglamai yÃ¶nlendirme burada yapÄ±lÄ±r.

### API Gateway GÃ¶revleri

| `GÃ¶rev`                       | `AÃ§Ä±klama`                                                 |
| ----------------------------- | ---------------------------------------------------------- |
| Routing                       | /api/users -> UserService, /api/payments -> PaymentService |
| Authentication/ Authorization | JWT Token kontrolÃ¼, OAuth 2.0                              |
| Rate Limiting                 | Brute Force veya DDOS korumasÄ±                             |
| Load Balancing                | AynÄ± servisin birden fazla instance' Ä±na istekleri daÄŸÄ±tÄ±r |
| Caching                       | SÄ±k istekleri Ã¶nbellekten dÃ¶ner                            |
| Monitoring/ Logging           | Her isteÄŸin sÃ¼resini, hatalarÄ±nÄ± izler                     |
| Transformation                | Header, body veya format dÃ¶nÃ¼ÅŸtÃ¼rme (JSON <-> XML)         |

### En YaygÄ±n API Gateway Ã‡Ã¶zÃ¼mleri

| `Gateway` | `AÃ§Ä±klama` | `Ã–nce Ã§Ä±kan Ã–zellik` |
| --------- | ---------- | -------------------- |

# Idempotency (TekrarlÄ± Ä°stekleri GÃ¼venli YÃ¶netme)

> Basit problem:

- KullanÄ±cÄ± 'SatÄ±n Al' butonuna bastÄ±.
- Ama internet yavaÅŸ -> sayfa dondu -> tekrar bastÄ± -> tekrar bastÄ±...

Sunucuya giden istekler:

1. SipariÅŸ OluÅŸtur.
2. SipariÅŸ OluÅŸtur.
3. SipariÅŸ OluÅŸtur.

EÄŸer sistem idempotent deÄŸilse:

- 3 sipariÅŸ oluÅŸur.
- 3 kere para Ã§ekilir.

### Idempotenct Nedir?

**AynÄ± istek kaÃ§ kere gelirse gelsin, sonuÃ§ sadece 1 kere oluÅŸur.**

```powershell
Request A == Request A == Request A
-> Tek bir iÅŸlem yaratÄ±r.
```

**Strip gibi Ã¶deme API' lerinde ÅŸÃ¶yle bir header bulunur.**

```makefile
Idempotency-Key: 87aksd9a8sd7asd9a
```

**MantÄ±k:**

- Ä°lk istek -> bu key ile iÅŸlem yapÄ±lÄ±r.
- AynÄ± key tekrar gelirse -> Ã¶nceki cevabÄ± dÃ¶ner.
- Yeni key gelirse -> yeni iÅŸlem

### Sistem NasÄ±l Kurulur?

Genelde: Redis veya Database kullanÄ±lÄ±r.

```pseudo

if(idempotencyKey exists in Redis):
    return storedResult
else:
    process request
    store result with key
    return result

```

1ï¸âƒ£ Senaryo:

Bir Ã¶deme servisi yapÄ±yorsun.

AynÄ± request 3 kere gelirse asla 3 kere para Ã§ekilmemeli.

ğŸ‘‰ Nerede idempotency uygularsÄ±n?

- Frontend?
- API Gateway?
- Backend Service?
- DB Layer?

1ï¸âƒ£ â€œIdempotency nerede uygulanmalÄ±?â€

âœ… AsÄ±l kontrol Backend Service katmanÄ±nda yapÄ±lmalÄ±.

**Neden ?**

- Frontend gÃ¼venilmez (manipule edilebilir).
- API Gateway sadece yÃ¶nlendirici (genelde iÅŸ mantÄ±ÄŸÄ± yok).
- DB tek baÅŸÄ±na yeterli deÄŸil.

**AltÄ±n Kural:**

> Idempotency her zaman business logic' in olduÄŸu yerde uygulanÄ±r.

### Node.js Idempotency Ã–rneÄŸi

Redis kullanÄ±yoruz **(standart yÃ¶ntem)**

```js
import Redis from "ioredis";
const redis = new Redis();

const processPayment = async (req, res) => {
  const { amount } = req.body;
  const idempotencyKey = req.headers["idempotency-key"];

  if (!idempotencyKey) {
    return res.status(400).json({ error: "Idempotency-Key required" });
  }

  const cacheKey = `idem:${idempotencyKey}`;

  //Kontrol -> (Daha Ã¶nce iÅŸlendi mi)
  const cached = await redis.get(cacheKey);
  if (cached) {
    console.log("ğŸ” Duplicate request detected");
    return res.json(JSON.parse(cached));
  }

  const result = {
    transactionId : cryto.randomUUID();,
    amount,
    status : "paid",
  };

  //Sonucu kaydet (30 dakika sakla)
  await redis.set(cacheKey, JSON.stringify(result), "EX",1800);
  console.log("âœ… Payment processed");
  return res.json(result);
};

```

**Client tarafÄ±nda nasÄ±l Ã§aÄŸrÄ±lÄ±r ?**

Ã–rnek headers: 

```http
POST /payment
Idempotency-Key : 7f9a9e21-1249-4d3c-b321-abc123456
```

AynÄ± key -> aynÄ± cevap
FarklÄ± key -> farklÄ± iÅŸlem

# Distributed Locking & Concurrency Control

Stok yÃ¶netimi, kritik alanlar, aynÄ± anda binlerce isteÄŸin yarÄ±ÅŸmasÄ± gibi gerÃ§ek hayattaki en zor problemleri kapsÄ±yor.

**Neden Concurreny Problemi OluÅŸur?**

10000 kiÅŸi aynÄ± anda: "Bu Ã¼rÃ¼nÃ¼ satÄ±n al"

O anda 10000 istek:

* AynÄ± Ã¼rÃ¼nÃ¼ okumaya Ã§alÄ±ÅŸÄ±r.
* AynÄ± stok deÄŸerini gÃ¶rÃ¼r.
* Stok deÄŸerini aynÄ± anda dÃ¼ÅŸÃ¼rmeye Ã§alÄ±ÅŸÄ±r.

Bu durum:

âŒ Overselling

âŒ Deadlock

âŒ DB lock

âŒ Race condition

gibi problemlere yok aÃ§ar.

## Race Condition

A ve B aynÄ± anda ÅŸu veriyi okuyor:

```
stock = 1
``` 
ve aynÄ± anda ÅŸu iÅŸlemi yapÄ±yor:

```
stock -= 1
```

`SonuÃ§:`

```
stock = -1
```

Yani sistemde fiziksel olarak olmayan bir Ã¼rÃ¼n satÄ±lmÄ±ÅŸ bulunuyor.

Bu olayÄ±n teknik adÄ± : **Race Condition**

## Overselling -> Race Condition' Ä±n sonucu

Race Condition yaÅŸanÄ±nca sonuÃ§ doÄŸal olarak:

**ÃœrÃ¼n stoktan fazla satÄ±lmÄ±ÅŸ oluyor.(overselling)**

Bu da mÃ¼ÅŸteri ÅŸikayetlerine yol aÃ§abilir.

1. Pessimistic Locking(DB level Lock)
2. Optimistic Locking (Version control/ row version)
3. Distributed Lock (Redis RedLock)
4. Single Writer Queue modeli
5. Atomic operations (Redis INCR/ DECR)

**Soru:** Bu bahsedilen stok sorunu yukarÄ±daki yÃ¶ntemlerden hangileri ile Ã§Ã¶zÃ¼lebilir.

**Cevap:** Db level locak yÃ¶nteminin Ã§ok saÄŸlÄ±klÄ± olduÄŸunu dÃ¼ÅŸÃ¼nmÃ¼yorum. redis redlock kullanÄ±labilir ama bence en makul Ã§Ã¶zÃ¼m single writer queue yapÄ±sÄ± gibi.

> DeÄŸerlendirme

### 1. DB Level Locking - (Neden saÄŸlÄ±klÄ± deÄŸil?)

Db Lock:

* SatÄ±rlarÄ± kilitler. (SELECT ... FOR UPDATE)
* AynÄ± anda binlerce istek geldiÄŸinde DB ÅŸiÅŸer.
* CPU ve I/O yÃ¼kselir.
* Lock queue bÃ¼yÃ¼r. -> sistem donar.
* Horizontal Scaling neredeyse imkansÄ±z

Bu yÃ¼zden high-load sistemlerde:

DB locking = `kÃ¶tÃ¼ fikir`
Monolitik ticari verilerde iÅŸe yarar ama bÃ¼yÃ¼k trafiklerde Ã§Ã¶ker.

### 2. Redis RedLock - (gÃ¼zel yÃ¶ntem ama riski)

RedLock ÅŸu iÅŸe yarar:

> Bu kaynaÄŸÄ± ben kilitledim, baÅŸkasÄ± dokunamaz.

Ama riskleri:

* Redis node dÃ¼ÅŸerse lock kaybolabilir.
* Clock drift (zaman kaymasÄ±) sorun Ã§Ä±karabilir.
* Netflix, AWS gibi firmalar RedLock' u Ã¶nermez.
* Sadece best-effort lock' tÄ±r -> kritik finansal iÅŸlemlerde risklidir.

RedLock = iyi ama kritik sistemlerde yeterince gÃ¼venli deÄŸil.

### 3. Single Writer Queue (En doÄŸru Ã§Ã¶zÃ¼m)

Bu yÃ¶ntem Amazon, Uber, Shopify, Etsy gibi devlerde ÅŸu ÅŸekilde kullanÄ±lÄ±r:

> "Stok dÃ¼ÅŸme gibi kritik bir iÅŸlemi aynÄ± anda sadece bir iÅŸlemci yapar."

**NasÄ±l Ã‡alÄ±ÅŸÄ±r?**

1. KullanÄ±cÄ± "satÄ±n al" -> event oluÅŸturur.
2. Event -> Queue(Kafka/ RabbitMQ/ SQS) iÃ§ine atÄ±lÄ±r.
3. Queue iÃ§erisindeki eventlar tek bir worker tarafÄ±ndan sÄ±rasÄ±yla iÅŸlenir.
4. AynÄ± anda 10.000 kiÅŸi bastÄ±ÄŸÄ±nda bile sistem iÅŸlem sÄ±rasÄ± bozulmaz.

Bu yapÄ±:

* Overselling = 0 
* Lock = yok
* DeadLock = yok
* DB yÃ¼kÃ¼ = minimum
* Vertical/ Horizontal Scaling = kolay
* Tam deterministik davranÄ±ÅŸ

#### Small Insight

Single writer queue iÅŸlem sÄ±rasÄ±nÄ± garanti eder.

```
Event 1 -> Stok = 10 -> `Process` -> Stok = 9
Event 2 -> Stok = 9 -> `Process` -> Stok = 8
Event 3 -> Stok = 8 -> `Process` -> Stok = 7
Event 4 -> Stok = 7 -> `Process` -> Stok = 6
```

Synchronize, gÃ¼venli, deterministik.

High-Load sistemlerde en bÃ¼yÃ¼k problem: 

âœ” SÄ±ra

âœ” TutarlÄ±lÄ±k

âœ” Ã‡akÄ±ÅŸma engelleme

Single writer queue bunlarÄ±n hepsini Ã§Ã¶zer.

##### âœ”ï¸ Single Writer Queue hangi durumda kÃ¶tÃ¼ Ã§Ã¶zÃ¼m olur?

**1. Ä°ÅŸlemler aÄŸÄ±rsa -> (Kuyruk taÅŸar)**

Her sipariÅŸ iÅŸlemi 10 saniye sÃ¼rÃ¼yor olsun.

```
10.000 sipariÅŸ / 10saniye = 100.000 saniyelik backlog
â‰ˆ 27 saat gecikme
```
`Deadletter dolup taÅŸar.`

**2. Event Ã§ok fazlaysa -> (Worker yetiÅŸemez)**

Amazon Ã¶lÃ§eÄŸi gibi:

```
1 milyon event / saniye
```

`Tek worker bunu yetiÅŸtiremez -> saniyede maks 3-5k event iÅŸler.`

**3. Throughput dÃ¼ÅŸÃ¼k kalÄ±r**

Worker tek olduÄŸu iÃ§in:

```
Max throughput = Worker' Ä±n CPU gÃ¼cÃ¼
```

`Daha fazlasÄ±nÄ± iÅŸleyemez -> bottleneck oluÅŸur.`

### Optimistic Locking (Versioning) Nedir? 

Bu yÃ¶ntem:
> Kilitlemeden veri gÃ¼ncelle, ama gÃ¼ncelleme anÄ±nda veri deÄŸiÅŸmiÅŸse iÅŸlemi reddet.

| `Id` | `Stock` | `Version` |
| ---- | ------- | --------- |
| 5 | 10 | 1 |

Ä°stemciler gÃ¼ncelleme ÅŸÃ¶yle yapÄ±lÄ±r:

```sql
UPDATE products
SET Stock = Stock -1 , Version + 1
WHERE Id = 5 AND Version = 5
```

EÄŸer 2 iÅŸlem aynÄ± anda buna vurursa:

* Sadece ilk iÅŸlem baÅŸarÄ±lÄ± olur.
* DiÄŸerinin WHERE koÅŸulu tutmadÄ±ÄŸÄ± iÃ§in 0 row updated olur

Bu da ÅŸunu garanti eder:
- Overselling olmaz
- DB lock gereksiz
- Concurrency Ã§ok temiz Ã§Ã¶zÃ¼lÃ¼r

Optimistic Locking iÅŸleminin baÅŸarÄ±sÄ±z olduÄŸu durumlar:

**1. Stok Ã‡ok HÄ±zlÄ± deÄŸiÅŸiyorsa -> Conflict PatlamasÄ±**

```
stock = 10
aynÄ± anda 10.000 kiÅŸi satÄ±n alÄ±yor.
```

Bu durumda:

* 10 kiÅŸi baÅŸarÄ±lÄ± olur.
* 9.990 kiÅŸi `version mismatch` hatasÄ± alÄ±r.
* Retry -> yine mismatch
* Tekrar retry -> yine mismatch

- Conflict oranÄ± Ã§ok yÃ¼ksek
- Gereksiz CPU harcanÄ±r
- TÃ¼m sistem "retry storm" yaÅŸar

**Optimistic Locking hÄ±zlÄ± deÄŸiÅŸen verilerde verimsizdir.**

**2. "Retry Loop" Ã§ok artarsa -> Sistem Ã§Ã¶kebilir**

Optimistic Locking baÅŸarÄ±sÄ±z olduÄŸunda client genelde ÅŸu ÅŸekilde Ã§alÄ±ÅŸÄ±r.

```js
while(true){
    try update;
    if(!success)
        retry with delay;
}
```

High Load altÄ±nda:
* 1 hata -> 100 retry doÄŸurur
* 100 hata retry
* Retry storm -> sistem Ã§Ã¶ker

Bu yÃ¼zden bÃ¼yÃ¼k ÅŸirketlerde retry iÃ§in `exponential backoff + jitter` ÅŸarttÄ±r.

**3. Version conflict oranÄ± yÃ¼kselirse -> Throughput dÃ¼ÅŸer**

> High Conflict = DÃ¼ÅŸÃ¼k Throughput

* Her saniye 10.000 iÅŸlem yarÄ±ÅŸÄ±yor.
* Ama database sadece saniyede 500 gÃ¼ncelleme iÅŸleyebiliyor.
* Geri kalan 9500 iÅŸlem retry yapmak zorunda.

Buda :

- Queue bÃ¼yÃ¼r.
- CPU artar.
- DB CPU tavan yapar.
- Gecikmeler fÄ±rlar.

`Ã‡ok yÃ¼ksek trafik = Optimistic Locking verimli Ã§alÄ±ÅŸmaz.`

Bu yÃ¼zden dev sistemler Optimistic Locking' i tek baÅŸÄ±na kullanmaz.

Genelde: 

- Redis atomic ops
- Queue-based writer
- Idempoteny 
- Optimistic Locking

kombinasyonlarÄ±nÄ± kullanÄ±rlar.




