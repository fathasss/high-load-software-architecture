# High Load System

√áok fazla kullanƒ±cƒ±dan gelen istekleri hƒ±zlƒ±, g√ºvenilir ve bozulmadan kar≈üƒ±layabilen sistemlerdir.

1. High Load Sistemlerin Ana Hedefleri

- Scalability (√ñl√ßeklenebilirlik): Trafik arttƒ±k√ßa sistemin √ß√∂kmesi yerine b√ºy√ºyerek cevap verebilmesidir. (√∂rn: Cloud provider)

- Availability (Eri≈üilebilirlik): Sistem √ßalƒ±≈ümaya devam etmeli, kesinti olmamalƒ±.

- Reliability (G√ºvenilirlik): Veriler kaybolmamalƒ±.

## Realistic Senario:

Diyelim ki kendi URL Shortener sistemimizi yapƒ±yoruz. (bit.ly gibi)

- G√ºnl√ºk 10 milyar yeni link olu≈üturuyor.
- G√ºnl√ºk 2 milyar tƒ±klama geliyor.

Bu ne demek ? 

```
G√ºnde 2.000.000.000 tƒ±klama
-> Saniye yakla≈üƒ±k 23.148 istek (avarage)
-> Pik zamanlarda bu 5x' e √ßƒ±kar. ~115.000 istek/saniye
```

üëâ Bu y√ºk√º tek bir sunucu kaldƒ±ramaz. ƒ∞≈üte bu y√ºzden High Load mimarisi gerekir.

**High Load Sistemlerin olayƒ± sadece g√º√ßl√º donanƒ±m deƒüil, trafiƒüi doƒüru y√∂netebilmektir.**

---

# Scaling Y√∂ntemleri

Y√ºksek trafikli sistemler kurgularken ilk b√ºy√ºk karar ≈üudur: 

**G√ºc√º artƒ±rarak mƒ± √∂l√ßeceƒüim, yoksa sayƒ±yƒ± artƒ±rarak mƒ± ?**

1. Vertical Scaling (Scale Up) - "Sunucuyu B√ºy√ºt"

- Var olan sunucuya daha fazla RAM/CPU/Disk ekliyorsun.
- Kolaydƒ±r ama sƒ±nƒ±rƒ± vardƒ±r.

Avantajlarƒ±:
- Y√∂netmesi kolaydƒ±r. 
- Kodda deƒüi≈üiklik gerekmez.

Dezavantajlarƒ±: 

- Bir noktadan sonra en b√ºy√ºk sunucuyu bile satƒ±n alamazsƒ±n.
- Tek bir failure point -> Sunucu √ß√∂kerse t√ºm sistem komple gider.

2. Horizontal Scaling (Scale Out) - "Sunucuyu √áoƒüalt"

- Tek sunucuyu b√ºy√ºtmek yerine aynƒ± sunucudan 10,100,100 tane koyulmasƒ±.
- ƒ∞steklerin Load Balancer ile  daƒüƒ±tƒ±lmasƒ±.

### Load Balancer (Y√ºk Dengeleyici)

Horizontal Scaling' in kalbi. M√º≈üteriden gelen istekleri alƒ±r, sunucularƒ± e≈üit ≈üekilde daƒüƒ±tƒ±r.

```
Client-> Load Balancer -> Server1 / Server2 / Server3

```

**Pop√ºler Load Balancer' lar :**

| `Yazƒ±lƒ±m Bazlƒ±` | `Cloud Managed` |
| --- | --- | 
| nginx,HAProxy |  AWS ELB/ ALB, Google LB, Azure LB  |

> Vertical Scaling neden uzun vadede s√ºrd√ºr√ºlebilir deƒüildir?

```
Veritical scaling uzun vadeli s√ºrd√ºr√ºlebilir olmamasƒ±nƒ±n sebebi her sunucu bilgisayarƒ±nƒ±n b√ºy√ºt√ºlmesi i√ßin bir limit olmasƒ±dƒ±r.
```

> Horizontal Scaling kullanƒ±rken tek bir kritik bile≈üen vardƒ±r ‚Äî o nedir?

```
Horizontal Scaling kullanƒ±rken kritik bile≈üen Load Balancer' dƒ±r. 
```

> Load Balancer'dan sonra sistemde stateful olmanƒ±n neden sorun olabileceƒüini tahmin edebiliyor musun?

```
√á√ºnk√º istekler her seferinde farklƒ± sunucuya gidebilir ve sunucu i√ßinde tutulan session / kullanƒ±cƒ± bilgisi kaybolur.
```

---

# Stateless vs Stateful Sistemler 

Load Balancer ile sunucular √ßoƒüaltƒ±ldƒ±ƒüƒ±nda ≈ü√∂yle bir durum olu≈üur.

```
Client -> Load Balancer -> Server A
Client -> Load Balancer -> Server B
Client -> Load Balancer -> Server C

```

Eƒüer bir kullanƒ±cƒ±yla ilgili oturum (login bilgisi, sepet, ge√ßici i≈ülem verisi) sunucunun i√ßinde saklanmƒ±≈üsa buna : 

**Stateful Server** (durum tutan sunucu) denir.

Problem ≈üudur:

* Kullanƒ±cƒ± ilk istekte Server A' ya d√º≈üer i login olur (session Server A' da tutulur.)
* Sonraki istek Server B' ye d√º≈üerse -> kullanƒ±cƒ± yeniden login olmak zorunda kalƒ±r.

**ƒ∞≈üte bu y√ºzden High Load sistemlerde 'State' sunucuda saklanmaz. Ortak bir yere alƒ±nƒ±r.**

Bu sisteme **Stateless Architecture** denir.


> Load Balancer'dan sonra sistemde stateful olmanƒ±n neden sorun olabileceƒüini tahmin edebiliyor musun?

**√á√∂z√ºm:**

| `State Saklama Yeri` | `A√ßƒ±klama`                                                                 |
|  ---                 |  ---                                                                       |
| Redis                | En √ßok kullanƒ±lan shared session store                                     |
| Database             | Kullanƒ±cƒ± bilgisi DB' de tutulabilir                                       |
| JWT/ Token           | Durumu client tarafƒ±na encode edip geri g√∂nderirsin. (sunucu state tutmaz) |

### üîÅ Bonus: Sticky Session Nedir?

Eƒüer stateless yapƒ± kurmak istemiyorsan Load Balancer' a "Bu kullanƒ±cƒ± hep aynƒ± sunucuya d√º≈üs√ºn." diyebilirsin. Buna **sticky session/session affinity** denir.

**Ama High Load sistemlerde tavsiye edilmez, √ß√ºnk√º sunucu √∂l√ºrse session' da gider.**

Stateful ve Stateless sorununun somut bir √∂rneƒüi

## √ñrnek E-ticaret Sitesinde Sepet Sistemi:

Sen bir e-ticaret sitesi yapƒ±yorsun (HepsiBurada/ Trendyol gibi). Kullanƒ±cƒ± √ºr√ºne tƒ±klayƒ±p **sepete ekle** diyor. 

- üß® Hatalƒ± Stateful Mimarisi:

```
Client -> Load Balancer -> Server A (Sepeti RAM' de tutuyor.)
```

Kullanƒ±cƒ± ikinci √ºr√ºn√º eklediƒüinde: 

```
Client -> Load Balancer -> Server B (Bu sunucuda sepet bo≈ü!)
```

> Kullanƒ±cƒ± "Sepetim kayboldu" diye √ßƒ±ldƒ±rƒ±r. √á√ºnk√º state(durum) sunucuya kilitli kalmƒ±≈ütƒ±r.

- ‚úÖ Doƒüru (Stateless) Mimarisi:

```
Client -> Load Balancer -> Server X -> Sepeti Redis/ DB/ Token' a kayƒ±t eder.
Client -> Load Balancer -> Server Y -> Sepeti Redis' ten okur.
```

**Hangi sunucuya d√º≈üerse d√º≈üs√ºn, client' ƒ±n durumu ortak bir yerde tutulur.**

* 1Ô∏è‚É£ : Bu √∂rnekte State'i merkezi tutmak i√ßin en mantƒ±klƒ± yer neresi olabilir ? 

1. Database(Sepet tablosu i√ßersinde tutmak)
2. Redis(Hƒ±zlƒ± cache depolama)
3. JWT Token(Sepeti client tarafƒ±na encode edip cookie i√ßerisinde ta≈üƒ±mak)

* Cevap : 
```
Ben 2. cevabƒ± se√ßerdim redis' te tutmak daha mantƒ±klƒ±. Database' de tutmak da mantƒ±klƒ± olabilir.
Ancak tek mantƒ±ksƒ±z olan yol token' da saklamak olduƒüunu d√º≈ü√ºn√ºyorum.
√á√ºnk√º token s√ºresinin belirli expire s√ºresi var ve bu s√ºre belki 1 hafta sonra dolabilir ve sepet datasƒ± kaybolabilir.
```

**Cevap deƒüerlendirmesi:** 

| `Se√ßenek`          | `Avantaj`                              | `Dezavantaj`                                                            | `Deƒüerlendirme`                             |
| ---                | ---                                    | ---                                                                     | ---                                         |
| DB' de saklamak    | Kalƒ±cƒ±, g√ºvenli                        | Yava≈ü (her sepet aksiyonunda DB sorgusu d√∂ner -> y√ºk bindirir.)         | Mantƒ±klƒ± ama aƒüƒ±r olabilir.                 |
| Redis' te saklamak | √áok hƒ±zlƒ±(in-memory) TTL ayarlanabilir.| Redis √ß√∂kse data kaybolabilir -> **ama backup/replication yapƒ±labilir** | Sepet gibi "ge√ßici state" i√ßin ideal tercih |
| Token' da saklamak | Sunucusuz, stateless                   | Token ≈üi≈üer (b√ºy√ºk payload), expire olursa sepet kaybolur.              | Riskli                                      |

**Yani Redis, √º√ß√ºn√ºn arasƒ±nda en dengeli olanƒ±** -- hem stateless sistemi destekler, hem de y√ºksek trafikte hƒ±zlƒ± √ßalƒ±≈üƒ±r.

---

# Cache & Database Stratejileri (Redis+DB ƒ∞li≈ükisi)

Bu derste ≈üu netle≈ütirilecek.

> "Veri nereye yazƒ±lmalƒ±? Cache nereye, database nereye? Hangisi √∂nce okunmalƒ±?"

**3 √ße≈üit cache stratejisi vardƒ±r.**

1. **Read-Through Cache:** Client DB' ye gitmez. -> Cache' e gider. Cache' de yoksa DB' den √ßeker. (Kullanƒ±cƒ± verileri, profil bilgisi vb.)
2. **Write-Through Cache:** Yazƒ±lan her veri √∂nce cache' e, sonra DB' ye yazƒ±lƒ±r. (Kalƒ±cƒ± olmasƒ± gereken data)
3. **Write-Back (Write-Behind):** Veri √∂nce sadece cache'e yazƒ±lƒ±r, sonra arkadan DB' ye flush edilir. (Sepet/ Like sistemi gibi ge√ßici, toleranslƒ± veriler)

Sistem ve kullanƒ±lan cache stratejisi olarak incelersek: 

| `Sistem` | `Cache Stratejisi`|
| --- | --- | 
| User Profile | **Read-Through** (cache varsa ordan oku, yoksa DB' den √ßek.) |
| Sepet | **Write-Back** (Redis'te tut -> Db' ye periyodik yazabilirsin veya hi√ß yazmayabilirsin.) |
| Banka Bakiyesi | Asla cache yazarken DB' ye gecikmeli gitmemeli -- **Write-Through** yada direkt DB  |

- Cache, verinin kopyasƒ±dƒ±r. Database' in yerine ge√ßmez.
- Redis gibi cache' ler volatile (u√ßucu) olabilir; tamamen kritik olan veriler i√ßin DB garanti noktasƒ±dƒ±r.
- Sepet gibi ge√ßici ama hƒ±zlƒ± olmasƒ± gereken ≈üeyleri Redis' te tutmak en makul yoldur.

**Sepet sipari≈ü mantƒ±ƒüƒ±nda 'Sepet = ge√ßici state', 'Sipari≈ü = kalƒ±cƒ± state' olarak deƒüerlendirilebilir.**

---

# Replication & Consistency (Master-Slave/ Primary-Replica/ Strong vs Eventual)

≈ûimdi sistemleri √ßoƒüalttƒ±k (scaling), cache' e aldƒ±k (Redis). Sƒ±radaki soru: 

> Database' i nasƒ±l √∂l√ßeklendiririz?
> Yani High Load altƒ±nda okumalar ve yazmalar nasƒ±l daƒüƒ±tƒ±lƒ±r? 

## Database Replication Nedir ? 

Bir DB' yi birden fazla kopya olarak tutmak demektir.

**Roller:**
1. Primary(Master): T√ºm yazmalar (INSERT/UPDATE) buraya yapƒ±lƒ±r.
2. Replica(Slave/Read Replica): Sadece okumalar yapƒ±lƒ±r.(SELECT) 

Yazma/ Okuma Ayrƒ±mƒ± B√∂yle √áalƒ±≈üƒ±r: 

```
Client(Write) -> Primary DB
Client(Read)  -> Replica DB' ler
```

B√∂ylece y√ºk payla≈üƒ±lmƒ±≈ü olur.

‚ö†Ô∏è Ancak burada bir sorun var : **Replication Lag**

- Primary DB' ye veri yazƒ±lƒ±r.
- Replica bu veriyi hemen mi alƒ±r? -> *Hayƒ±r, √ßoƒüu zaman gecikmeli gelir.*

Bu y√ºzden ortaya iki tip tutarlƒ±lƒ±k modeli √ßƒ±kar:

1. **Strong Consistency:** Yazƒ±lan veri anƒ±nda t√ºm DB' lerde g√∂r√ºn√ºr. (√ñrn: Bankacƒ±lƒ±k Sistemleri) 
2. **Eventual Consistency:** Veriler kƒ±sa s√ºreliƒüine farklƒ± olabilir, ama sonunda e≈üitlenir. (√ñrn: Sosyal Medya like sayƒ±larƒ±)

### Eventual Consistency' nin Klasik √ñrnekleri:

Sen instagramda bir postu like' ladƒ±n.

- Hemen altta "12502 -> 12503 beƒüeni" olur.
- Arkada≈üƒ±n telefonunda hala 12502 g√∂r√ºn√ºyor.

**Bu bir bug deƒüil eventual consistency' dir.**

* 1Ô∏è‚É£: Bir e-ticaret sitesinde "√ºr√ºn stok sayƒ±sƒ±" sence hangi modelle tutulmalƒ±? (Strong mu, Eventual mƒ±?) Neden ? 

* Cevap: Bir e-ticaret sitesinde √ºr√ºn√ºn stok sayƒ±sƒ± strong consistency ile tutulabilir. 
          √á√ºnk√º sepete ekleyip sipari≈ü verildiƒüinde eƒüer bir gecikme ya≈üanƒ±rsa sonrasƒ±nda sipari≈üin iptali gibi
          m√º≈üterilerde olumsuz durumlara yol a√ßma riski vardƒ±r.

* 2Ô∏è‚É£: **Kullanƒ±cƒ± profiline ait g√∂sterilen takip√ßi sayƒ±sƒ±** sence strong consistency mi gerektirir yoksa eventual olabilir mi?

* Cevap: Kullanƒ±cƒ± profiline ait g√∂sterilen takip√ßi sayƒ±sƒ± eventual consistency ile i≈ülem yapƒ±lmasƒ± daha saƒülƒ±klƒ± olur. 
         √á√ºnk√º 1. cevapdaki gibi √ßok √ßok olumsuz durumlara ve sonrasƒ±nda ya≈üanacak krizlere sebebiyet verecek bir √∂ncelik olmadƒ±ƒüƒ±nƒ± d√º≈ü√ºn√ºyorum.

**Kritik olmayan datalarda Eventual Consistency, kritik datalarda Strong Consistency se√ßimi yapƒ±lƒ±r. High Load sistem tasarƒ±mƒ± tam da bu dengeyi kurmak demektir.**

---

# Partitioning/ Sharding (Ger√ßek √ñl√ßeklenme Burada Ba≈ülar.)

Cache ekledik. Replication yaptƒ±k ama bir noktada tek bir Primary DB bile yetmeyecek.

ƒ∞≈üte bu durumda yapƒ±lacak ≈üey: 

> Database'i b√∂lmek.

## Partitioning/ Sharding Nedir? 

**Devasa bir tabloyu veya database' iki ya da daha fazla par√ßaya b√∂lmek.**

B√∂ylece:

```
instread of -> 1 DB with 1M users
we do -> 10 DB each with 10K users
```

### Sharding Y√∂ntemleri:

1. **Range Sharding:** Alfabetik veya ID aralƒ±ƒüƒ±na g√∂re *(√ñrn: A-M -> DB1, N-Z -> DB2)*
2. **Hash Sharding:** ID' yi hash' leyip mod alarak *(√ñrn: user_id % 4 -> 4 farklƒ± shard)*
3. **Geo Sharding:** Lokasyona g√∂re *(√ñrn: Avrupa ayrƒ± DB, Amerika ayrƒ± DB)*
4. **Feature/Entity Based:** Veri tipine g√∂re *(√ñrn: dbo.Users tablosu farklƒ±, dbo.Orders farklƒ± DB' de )*

* √ñrnek: Kullanƒ±cƒ±larƒ± Hash ile Shard Et:

```
Shard = user_id % 4 

user_id = 341 => 341 % 4 = 1 -> Database A 
user_id = 762 => 762 % 4 = 2 -> Database B

```

**Her sunucu sadece kendi par√ßasƒ±nƒ± bilir -> y√ºk daƒüƒ±tƒ±lƒ±r,sorgular hƒ±zlanƒ±r.**

**Sharding Sorunlarƒ±:**

1. **Cross-Shard Query:** "Bir kullanƒ±cƒ±nƒ±n 5 sipari≈üi, 3 yorumu ve 2 adresi var." -> hepsi farklƒ± DB' de ise toplamak zorla≈üƒ±r.
2. **Shard Rebalancing:** "Shard 1 √ßok doldu,2 bo≈ü kaldƒ±" gibi durumlarda yeniden daƒüƒ±tƒ±m gerektirir.
3. **ID Uniqueness:** Farklƒ± shard' lardaki veriler √ßakƒ±≈üabilir. -> *global_id* sistemi gerekebilir.

* 1Ô∏è‚É£: Sence ‚ÄúKullanƒ±cƒ± tablosu‚Äùnu sharding yapmak mantƒ±klƒ±dƒ±r ama ‚Äú√úr√ºn kategorileri‚Äù tablosunu sharding yapmak mantƒ±klƒ± mƒ±dƒ±r? Neden?

* Cevap: Bence √ºr√ºn kategorileri tablosunu sharding yapmak √ßok deƒüildir. √á√ºnk√º kategorilerin mutlak bir sƒ±nƒ±rƒ± vardƒ±r. Bu y√ºzden datanƒ±n b√∂l√ºnmesi gerektiƒüini d√º≈ü√ºnm√ºyorum.

* 2Ô∏è‚É£: Sharding yapƒ±ldƒ±ƒüƒ±nda "user_id = 1234" i√ßin hangi shard kullanƒ±labileceƒüini nasƒ±l bilebiliriz. (Senin √∂nerdiƒüin y√∂ntem)

* Cevap: √áok b√ºy√ºk bir trafik d√º≈ü√ºnd√ºƒü√ºm√ºzde mesela amazon gibi. √ñnce Geo Sharding y√∂ntemi ile b√∂lgelere ayƒ±rƒ±p ardƒ±ndan aynƒ± b√∂lgelerdeki kullanƒ±cƒ±lar i√ßin de Hash Sharding kullanarak daha verimli bir b√∂l√ºmleme ile sistemin daha stabil ve hƒ±zlƒ± √ßalƒ±≈üabileceƒüini d√º≈ü√ºn√ºyorum.
Mesela T√ºrkiye' de kayƒ±t olan bir amazon kullanƒ±cƒ±sƒ± Geo Sharding y√∂ntemi ile TR sunucularƒ±na kayƒ±t olacak ardƒ±ndan TR' de bulunan istanbul, Ankara veya ƒ∞zmir gibi illerde bulunan sunucularƒ±nda Hash Sharding kullanarak veri payla≈üƒ±mƒ± yapmak daha saƒülƒ±klƒ± olabilir. Cevap Amerika i√ßin yine Amazon gibi y√ºksek trafikli bir sistemden bahsedecek olursak ilk olarak Geo sharing ile kƒ±ta bazlƒ± ABD sunucusu olarak b√∂ler sonrasƒ±nda yine Geo sharding ile bu sefer ABD i√ßerisindeki sunucularƒ± eyalet bazlƒ± b√∂lerdim ve eyalatlerde bulunan sunucularƒ± Hash Sharding ile b√∂lmeyi d√º≈ün√ºrd√ºm. Mesela Washington i√ßin aktif 30 sunucu varsa bu 30 sunucu da Hash Sharding y√∂ntemi uygulardƒ±m. 

> **Eƒüer data √ßok k√º√ß√ºkse -> b√∂lmek yerine t√ºm sunuculara kopyalamak daha mantƒ±klƒ±dƒ±r. (Replication)**

> **Eƒüer data b√ºy√ºkse -> par√ßalamak zorundayƒ±z. (Sharding)**

---

# Message Queues & Asenkron ƒ∞≈ülem (Event Driven Architecture)

## ‚úÖ Message Queue Nedir?

> √úretilen olaraylarƒ± (event) *hemen i≈ülemek sƒ±raya koymak* ve asenkron **(gecikmeli ama garantili)** ≈üekilde i≈ülemek i√ßin kullanƒ±lan sistemdir.

### üìåNeden Kullanƒ±lƒ±r?

√á√ºnk√º ger√ßek d√ºnyada bazƒ± i≈ülemler hemen yapƒ±lmak zorunda deƒüildir.

| `ƒ∞≈ülem` | `Senkron Durumu` |
| --- | --- |
| Kullanƒ±cƒ± 'Pay' butonuna bastƒ± - > √ñdeme Onayƒ± | Senkron olmalƒ±(Hemen cevap vermeli) |
| √ñdemeden sonra 'Mail G√∂nder' | Asenkron (Sƒ±ra kuyruƒüa yaz, arkadan i≈ülenir.) |
| Like sayƒ±sƒ±nƒ± artƒ±r | Asenkron yapƒ±labilir | 
| Kullanƒ±cƒ±ya bildirim g√∂nder | Kuyruƒüa atƒ±labilir |
| b√ºy√ºk resmi sƒ±kƒ±≈ütƒ±r. | Kuyruƒüa at |

### Queue Kullanmazsan Ne Olur ?

√ñrnek: 

```
POST /buy

-> DB insert
-> Stok azalt
-> Mail g√∂nder
-> PDF Fatura olu≈ütur
-> Bildirim g√∂nder
-> Slack mesajƒ± g√∂nder
```

Hepsini aynƒ± anda yaparsan 5 saniye s√ºrer. Kullanƒ±cƒ± "bozuldu mu?" diye sayfayƒ± kapatƒ±r.

### Queue ile Doƒüru Mimari

```
POST /buy
-> DB insert + Stok d√º≈ü (sadece ana i≈ülem)
-> Event "OrderCreated" kuyruƒüa yazƒ±lƒ±r
-> Worker' lar arkadan: Mail, Fatura, Bildirim, Slack g√∂nderir.
```

Kullanƒ±cƒ±ya cevap verilir. -> **Sipari≈ü Alƒ±ndƒ±**

Arka planda her≈üey devam eder.

**Message Queue √ñrnekleri:**

| `Sistem` | `Type` | `Kullanƒ±m` |
| --- | --- | --- |
| RabbitMQ | Message Broker | K√º√ß√ºk-Orta Sistemler |
| Kafka | Event Streaming | Y√ºksek throughput -> 1M+ events | 
| AWS SQS, GCP Pub/Sub | Cloud Queue | Serverless kullanƒ±m |
| Redis Stream | Lightweight Queue | Basit ƒ∞≈üler i√ßin |

* 1Ô∏è‚É£: Bir e-ticaret sitesinde "Sipari≈ü verildiƒüinde mail g√∂nderme" i≈ülemi sence Senkron mu olmalƒ±, Asenkron mu? Neden ?

* Cevap: Bence asenkron olmalƒ±. Senkron bir sipari≈ü akƒ±≈üƒ± tasarlanmƒ±≈ü ve canlƒ± data yazƒ±lmƒ±≈ü. Kullanƒ±cƒ± artƒ±k sipari≈üini vermi≈ü ve gerekli stok i≈ülemleri yapƒ±lmƒ±≈ü. Bu y√ºzden mail kullanƒ±cƒ± i√ßin bir ek bilgilendirme. Kuyruƒüa alƒ±nƒ±p bekletilebilir.

* 2Ô∏è‚É£: Like butonuna basƒ±ldƒ±ƒüƒ±nda sayacƒ± hemen artƒ±rmak mƒ± gerekir, yoksa Queue' ya atƒ±p gecikmeli artƒ±rmak da olur mu? 

* Cevap: ƒ∞lk saya√ß i≈ülemi cache' de yapƒ±lƒ±p cache' deki sayacƒ±n artƒ±lƒ±rƒ±lƒ±p canlƒ± yazma i≈ülemi asenkron yapƒ±lmasƒ± daha saƒülƒ±klƒ± olur. √á√ºnk√º √ºnl√º bir influencer' ƒ±n sosyal medya postu saniyeler i√ßerisinde √ßok fazla trafik alabilir. Bu y√ºzden canlƒ± data olarak aktarma i≈ülemi asenkron olarak yapƒ±lmasƒ± ve kuyruk yapƒ±sƒ± ile kurgulanmasƒ± daha saƒülƒ±klƒ± olur. 

**ƒ∞kinci sorunun cevabƒ± i√ßin "Burst Traffic" ihtimalinin d√º≈ü√ºn√ºlm√º≈ü olmasƒ± gayet mantƒ±klƒ±. B√ºy√ºk sistemler (Instagram, Twitter) exactly this! y√∂ntemini kullanmaktadƒ±r.**

---

# Eventual Consistency Ger√ßek D√ºnyada Nasƒ±l Y√∂netilir?

Consistency konusuna √∂nce teorik olarak baktƒ±k ama ≈üimdi eventual consistency ile yapƒ±lan tutarsƒ±zlƒ±klar nasƒ±l y√∂netilir kƒ±smƒ±na ge√ßelim.

## Senaryo:

Bir e-ticaret sitesindesin.

- Kullanƒ±cƒ± Sipari≈üi Verdi -> √ñdeme Alƒ±ndƒ± ‚úÖ
- Ama arkaplanda √ßalƒ±≈üan asenkron i≈ülemlerden biri **BA≈ûARISIZ** oldu. (√∂rneƒüin stok g√ºncellemesi yapmadƒ±.)

| `Strateji` | `A√ßƒ±klama` | `Risk` |
| --- | --- | --- | 
| Strong Consistency (Transactional) | T√ºm adƒ±mlar aynƒ± anda yapƒ±lƒ±r, biri fail olursa geri alƒ±nƒ±r. | Kullanƒ±cƒ± bekler, yava≈ülatƒ±r. |
| Eventual Consistency + Retry Queue | Ba≈üaramayan event tekrar kuyruƒüa atƒ±lƒ±r ve yeniden denenir. | Bazƒ± event' lar *eninde sonunda* olur ama ge√ß olabilir. |
| SAGA Pattern (Compensating Action) | **Eƒüer stok g√ºncellemesi ba≈üarƒ±sƒ±z ise -> Sipari≈üi otomatik iptal et ve parayƒ± geri √∂de.** | Kullanƒ±cƒ±yƒ± √ºzmeden rollback. |

## SAGA Pattern (Daƒüƒ±tƒ±k Transactionlarƒ±n Kurtarƒ±cƒ±sƒ±)

| `Step` | `Process` | `Is Success?` | 
| --- | --- | --- |
| 1 | Payment-> Success | Compensate: Para iadesi | 
| 2 | Stok d√º≈ü | Compensate: Stok Geri Ekle | 
| 3 | Sipari≈üi DB' ye yaz | Compensate: Satƒ±r sil |
| 4 | Mail g√∂nder | (Gerek yok, sadece logla) | 

* 1Ô∏è‚É£ Eventual consistency kullanƒ±lan sistemlerde ‚Äúba≈üarƒ±sƒ±z event‚Äôler‚Äù i√ßin nasƒ±l bir strateji uygulanmalƒ±dƒ±r? Retry mi, kompanzasyon mu, loglayƒ±p bƒ±rakmak mƒ±? Sen nasƒ±l karar verirsin?

* Cevap: Ba≈üarƒ±sƒ±z event' lar i√ßin transactional i≈ülem uygulanabilir. Bir sipari≈üin verildiƒüini varsayalƒ±m. √ñdeme ve Stoktan d√º≈üme i≈ülemi Strong Consistency yapƒ±sƒ± ile yapƒ±lmƒ±≈ü olsun.
Kuyruk yapƒ±sƒ± kullanƒ±larak mail g√∂nderme gibi i≈ülemler s√∂z konusu. Kuyrukda hata alƒ±ndƒ±ƒüƒ±nda mail g√∂nderme, fatura kesme gibi i≈ülemler SAGA Pattern ile yapƒ±labilir.
Yada yalnƒ±zca √∂deme adƒ±mlarƒ± Strong Consistency ile yapƒ±ldƒ±ysa. Stoktan d√º≈üme, mail g√∂nderme ve fatura g√∂nderme gibi i≈ülemlerde SAGA Pattern kullanƒ±larak i≈ülemler geri alƒ±nabilir sipari≈ü iptal edilip para iadesi saƒülanabilir. ƒ∞ki yolda saƒülƒ±klƒ±dƒ±r. 

* 2Ô∏è‚É£ Sence bir bankacƒ±lƒ±k sisteminde ‚Äúpara transferi‚Äù eventual consistency olabilir mi? Yoksa strong mu olmak zorunda?

* Cevap: Bence bir bankacƒ±lƒ±k sisteminde para transferi i≈ülemi kuyruk yapƒ±sƒ± ile yapƒ±lmalƒ±. Yani Eventrual Consistency ve denenebilir kuyruk yapƒ±sƒ± kullanƒ±labilir. √á√ºnk√º bankacƒ±lƒ±k sistemlerinde para transferi yapƒ±lƒ±rken istek yapƒ±lan servislerde farklƒ± farklƒ± sorgulama servisleri √ßalƒ±≈üabilir ve bu trafiƒüin y√∂netilmesi i√ßin Eventual Consistency daha uygundur.

---

# Read-Heavy Trafikte Replication & CQRS

### Problem:

Bir sistemde: 

1. %10 Write (INSERT /UPDATE/ DELETE)
2. %90 Read (SELECT)

yapƒ±lƒ±yor olsun.

Bu durumda b√ºt√ºn read isteklerini primary database' den yapmak mantƒ±klƒ± mƒ±dƒ±r ? 

> Hayƒ±r. √á√ºnk√º primary s√ºrekli yazma da yapƒ±yor. -> Okuma y√ºk√º geldiƒüinde yava≈ülar.

### √á√∂z√ºm:

| `Veri` | `Nereden okunur?` | `Nereden yazƒ±lƒ±r?` | 
| --- | --- | --- |
| Yazma i≈ülemi (CREATE/ UPDATE) | -> Primary DB | ‚úÖ | 
| Okuma i≈ülemi (GET/ LIST) | -> Read Replica DB | ‚úÖ |

Bu yapƒ±ya √ßoƒüu zaman CQRS (Command Query Responsibility Segregation) denir. 

**Mimari:**

```
        WRITE   
App -------------> Primary DB
|
READ
|
-----------------> Read-Replica-1
|
-----------------> Read-Replica-2
```

* 1Ô∏è‚É£ Sadece **read heavy** olan bir endpoint (√∂rneƒüin: Trend √ºr√ºnleri listeleme) hangi DB‚Äôden okunmalƒ±? Primary mƒ±, Replica mƒ±?

* Cevap: Trend √ºr√ºnleri listelemek farklƒ± replica  database'lerden okunmasƒ± daha saƒülƒ±klƒ± olur. √á√ºnk√º trend √ºr√ºnler belirli ortalamaya g√∂re deƒüi≈üiklik g√∂sterebilir. Deƒüi≈üen bir veri olduƒüu i√ßin Replica' dan okumak daha saƒülƒ±klƒ± olabilir. 

* 2Ô∏è‚É£ Replica‚Äôdan okuduƒüumuz i√ßin ‚Äúanlƒ±k deƒüi≈üiklikleri 1-2 saniyelik gecikmeyle g√∂r√ºrsek‚Äù bu problem olur mu? Hangi senaryoda sorun olur, hangi senaryoda olmaz?

* Cevap: Sorun olabilecek senaryolardan bir tanesi anlƒ±k √ßok hƒ±zlƒ± deƒüi≈üen veriler. Mesela like sayƒ±larƒ± ya da √ºnl√º influencer' ƒ±n yaptƒ±ƒüƒ± payla≈üƒ±mƒ±n altƒ±nda gelen yorumlar veya reddit gibi platformlarda a√ßƒ±lmƒ±≈ü trend konularda etkile≈üimlerin yava≈ülamasƒ±nƒ± saƒülayabilir. Bu noktada Redis gibi bir yapƒ±nƒ±n kullanƒ±lmasƒ± daha saƒülƒ±klƒ± olur.

```
Primary DB(Write) -> Doƒüru ve G√º√ºncel Veri -> Ani y√ºkte zorlanƒ±r -> √ñdeme,Stok d√º≈üme
Read Replica -> Read' i √∂l√ßeklendirir -> 1-3 saniye gecikmeli olabilir -> Profil g√∂r√ºnt√ºleme, Trend √ºr√ºn listesi
Redis Cache -> Ultra Hƒ±zlƒ± Okuma -> Veri eski olabilir -> Like sayƒ±sƒ±, Hot Content
```

> Her≈üey DB' den **okunmaz**, her veri **aynƒ± ciddiyette deƒüildir.**

---

# Rate Limiting & Throttling (Kullanƒ±cƒ±larƒ± ve Botlarƒ± Kontrol Etme)

> Bir kullanƒ±cƒ± (veya bot) saniyede 1000 istek atarsa ne yaparsƒ±n ? 

1. **Rate Limit:** Belirli bir s√ºre i√ßinde bir kaynaƒüa ka√ß kez eri≈üilebileceƒüini kontrol eder. (1 IP -> dakikada en fazla 30 Login denemesi)
2. **Throttling:** Limit a≈üƒ±lƒ±rsa istekleri tamamen engellemek yerine yava≈ülatƒ±r. (Videoyu 1080P deƒüil 480P vermek)
3. **Quota:** Kullanƒ±cƒ±ya uzun d√∂nemli bir limit verir.  (Free User -> Ayda 5000 API √ßaƒürƒ±sƒ±)

## Rate Limiti Nasƒ±l Uygularƒ±z ? 

Rate limit uygulanabilecek 4 farklƒ± katman bulunur.

1. **Client Side (UI):**  "Bu butona hƒ±zlƒ± basmayƒ±n" uyarƒ±sƒ± -> Zayƒ±f ama iyi UX
2. **Backend Gateway/ API Gateway:** NGINX, Envoy, Kong, AWS API Gateway -> En sƒ±k kullanƒ±lan katman
3. **Middleware/ Code Level:** Express/ Nest/ Spring interceptor -> Esnek ama app y√ºklenir
4. **Firewall/ Network Level:** Cloudflare, AWS WAF -> Botlara kar≈üƒ± en etkili

> **Ger√ßek d√ºnya sistemlerinde Rate limit genelde Redis + Sliding Window/ Token Bucket algoritmasƒ±yla yapƒ±lƒ±r.**

* 1Ô∏è‚É£ Bir login endpoint‚Äôi i√ßin Rate Limiting uygulanmalƒ± mƒ±? Nasƒ±l bir limit koyardƒ±n?

* Cevap: Evet. Login endpoint' i i√ßin bir Rate Limit kullanƒ±rdƒ±m. √á√ºnk√º kaba kuvvet saldƒ±rƒ±sƒ±na maruz kalabilir. Bunun i√ßinde saniyede login endpoint'i i√ßin bir limiti olurdu ve bu limit a≈üƒ±ldƒ±ƒüƒ±nda gelen isteƒüin IP' sini kara listeye alƒ±rdƒ±m. 

* 2Ô∏è‚É£ API‚Äôyi kullanan premium ve free kullanƒ±cƒ±larƒ± farklƒ± ≈üekilde sƒ±nƒ±rlamak ister misin? Nasƒ±l?

* Cevap: Evet sƒ±nƒ±rlamak isterim. API' nin Premium s√ºr√ºm√º i√ßin ayda 100.000 API √ßaƒürƒ±sƒ± alabilecek ≈üekilde sƒ±nƒ±rlandƒ±rƒ±rƒ±m. Ayrƒ±ca Free s√ºr√ºm√º i√ßinde aylƒ±k 10.000 istek ile kƒ±sƒ±tlardƒ±m.

---

# Token Bucket Algoritmasƒ±

Rate limiting sistemlerde en √ßok kullanƒ±lan algoritma: 

> Token Bucket (ve varyasyonu: Leaky Bucket)

## Token Bucket Mantƒ±ƒüƒ±

Her kullanƒ±cƒ±ya bir bucket (kova) veriyoruz.

| `Kova √∂zelliƒüi` | `Anlamƒ±` |
| --- | --- |
| Kova kapasitesi (capacity) | Maksimum ka√ß istek hakkƒ± birikebilir ? |
| Token refill rate | Her saniye ne kadar yeni hak veriyoruz ? |
| Request Geldiƒüinde | Kovada token varsa -> ƒ∞zin verilir ve 1 token d√º≈ü√ºr√ºl√ºr. Yoksa -> Reddedilir veya bekletilir. |

* √ñrnek (Free Kullanƒ±cƒ±)

| `√ñzellik` | `Deƒüer` |
| --- | --- |
| Capacity | 100 Token |
| Refill Rate | 1 token/ saniye (yani dakikada 60 istek)

* Kullanƒ±cƒ± bir anda 100 istek atabilir. (kovada doluysa)
* Ama s√ºrekli atmak isterse en fazla 60 istek/ dakika yapabilir.

* √ñrnek (Premium Kullanƒ±cƒ±) 

| `√ñzellik` | `Free` | `Premium` | 
| --- | --- | -- |
| Capacity | 100 Token | 1000 token |
| Refill Rate | 1 t/s | 10 t/s|

* üëâ Login endpoint‚Äôi i√ßin hangi rate limiting modelini se√ßerdin?

1. Sliding Window (Zaman aralƒ±ƒüƒ±nda sabit limit ‚Äî √∂rn: 5 dakika i√ßinde en fazla 10 login denemesi)
2. Token Bucket (Saniyede 1 token ‚Äî anlƒ±k patlamaya izin verir ama s√ºrd√ºr√ºlebilir deƒüilse keser)
3. Fixed Window (Her 1 dakikada bir limit sƒ±fƒ±rlanƒ±r)

* Cevap: Eƒüer yalnƒ±zca bir tanesini kullanacak isem Sliding Window rate limit modelini se√ßerdim. √á√ºnk√º brute force kar≈üƒ± daha √∂nemli bir y√∂ntem olurdu. Ama eƒüer birden fazla kullanacaksam Sliding Window ve Token Bucket y√∂ntemlerini bir arada kullanabilirdim. Login endpoint' i i√ßin belirli sayƒ±da bir token olurdu mesela 3 token. ƒ∞lk 3 denemede parolayƒ± hatƒ±rlamadƒ±ysa kullanƒ±cƒ± Token Bucket' ta login endpoint i√ßin olan istekleri sƒ±fƒ±rlardƒ±m ve iki adƒ±mlƒ± doƒürulama kullanmasƒ±nƒ± isterdim. Burda da a≈üƒ±rƒ± istek oluyorsa ≈üayet sliding window ile belirlediƒüim limite g√∂re baƒülantƒ±yƒ± kapatƒ±rdƒ±m. 

---

# Circuit & Breaker Failover

* Neden Gerekli ?

Bir servise ya da dƒ±≈üa baƒüƒ±mlƒ± bir kaynaƒüa s√ºrekli ba≈üarƒ±sƒ±z istek atmak t√ºm sistemi yava≈ülatƒ±r veya √ß√∂kertir. **Circuit Breaker** bu t√ºr aksaklƒ±klarda aracƒ± olarak davranƒ±r: belirli bir ba≈üarƒ±sƒ±zlƒ±k e≈üiƒüine gelince daha fazla istek g√∂ndermeyi keser, sistemin geri kalanƒ±nƒ± korur. Failover ise arƒ±zalƒ± servisten trafiƒüi ba≈üka saƒülƒ±klƒ± servise y√∂nlendirmektir.

## Temel Kavramlar

1. **Timeouts:** Her isteƒüin makul bir s√ºre i√ßinde geri d√∂nmediƒüni d√º≈ü√ºnerek zaman a≈üƒ±mƒ± koyar.
2. **Retries with Backoff:** Hata alƒ±ndƒ±ƒüƒ±nda hemen tekrar denemek yerine artan gecikme (exponential backoff + jitter) uygula.
3. **Circuit Breaker States:** 
* Closed : Normal durum istekler g√∂nderilir.
* Open: Hatalar y√ºksek -> istekler reddedilir kƒ±sa s√ºreliƒüine.
* Half-Open: Belirli aralƒ±k sonra az sayƒ±da deneme g√∂nderilir; ba≈üarƒ±lƒ±ysa Closed' a d√∂ner, ba≈üarƒ±sƒ±zsa tekrar Open.
* Bulkhead: Kaynaklarƒ± b√∂l√ºmlere ayƒ±r; bir b√∂l√ºm √ß√∂kse diƒüerleri √ßalƒ±≈ümaya devam eder. (√∂rn: thread pool' lar).
* Failover/ Graceful Degradation: Bir hizmet √ß√∂kt√ºƒü√ºnde alternatif servis, cached response veya degraded ux sun.

### Tipik Circuit Breaker Kurallarƒ± (pratik)

* windowSize: son N istekte ne kadar hata yoksa?
* failureThreshold: % hata oranƒ± (√∂rn. %50) veya hatalƒ± istek sayƒ±sƒ± (√∂rn. 20) a≈üƒ±lƒ±rsa -> Open
* openTimeout: Open durumunda bekleme s√ºresi (√∂rn. 30s).
* halfOpenMaxCalls:  Half-open' dayken ka√ß istek test edilecek. (√∂rn. 5).
* successThreshold: Half-open testlerde ka√ß ba≈üarƒ±lƒ± istek ≈üart (√∂rn. 3) -> Closed.

**√ñrnek: PseudoCode (basit circuit breaker)**

```pseudo

class CircuitBreaker{
        state = "CLOSED"
        failures = 0
        successCount = 0
        lastFailure = null

        onRequest(call):
            if state == "OPEN":
                if now-lastFailureTime < OPEN_TIMEOUT:
                    throw CircuitOpenException
                else:
                    state = "HALF_OPEN"
                    successCount = 0

            try:
                resp = call()
                onSuccess()
                return resp
            except Exception:
                onFailure()
                throw

        onSuccess():
            if state == "HALF_OPEN":
                successCount += 1
                if successCount >= SUCCESS_THRESHOLD:
                    state = "CLOSED"
                    failures = 0
                else:
                    failures = 0

        onFailures():
            failures += 1
            lastFailuretime = now
            if  failures > FAILURE_THRESHOLD:
                state = "OPEN"
}

```
### Retries & Backoff (iyi uygulama)

1. **MaxRetries:** 3
2. **Backoff:** exponential (e.g., 100ms,  200ms, 400ms)
3. **Jitter:** k√º√ß√ºk rastgelele≈ütirme ekle, thundering herd √∂nlenir.
4. **Idempotency:** retry edilebilecek √ßaƒürƒ±lar idempotent olmalƒ± veya idempotency-key kullanƒ±lmalƒ±. (√∂rn: √∂deme i≈ülemleri i√ßin)

### Failover & Routing

1. **Active-Passive:** Primary' den hata alƒ±ndƒ±ƒüƒ±nda Secondary' ye y√∂nlendir.
2. **Active-Active:** Trafik her iki b√∂lgeye de gider; bir b√∂lge d√º≈üerse diƒüerleri kalƒ±r.
3. **Geo-Failover:** B√∂lge d√º≈üerse DNS veya global Load Balancer ile trafiƒüi ba≈üka b√∂lgeye ta≈üƒ±.

√ñrnek : DB read-replicas -> read from nearest replica; primary fail olursa promote replica veya route clients to other region.

### Bulkhead & Resource Isolation

- Her dƒ±≈ü API i√ßin ayrƒ± thread pool veya connection pool kullan.
- bir API yoƒüunla≈üƒ±rsa diƒüer API' nin kaynaklarƒ±nƒ± yemez.

### Graceful Degradation (Kullanƒ±cƒ±ya nasƒ±l davranƒ±rƒ±z?)

Eƒüer bir microservis d√∂nm√ºyorsa:

- Serve cached data (√∂nbellekleki son iyi sonu√ß)
- Minimal/ degraded UI (√∂rn. "≈üu an yorumlar y√ºkleniyor")
- Fail-fast: uzun bekletme yerine hƒ±zlƒ± fallback d√∂nd√ºr√ºr.

### Monitoring & Metrics (mutlaka izle)

Her circuit breaker i√ßin:

- requestRate
- errorRate
- latency (P50/ P95/ P99)
- state changes (Open->Half Open-> Closed)
- retry count
- downstream latency

Uyarƒ±lar(alert):

- Circuit Breaker open sayƒ±sƒ± artarsa (y√ºzde veya absolut)
- Latency P95 > hedef
- Retries > threshold

**Ger√ßek Hayatta Kullanƒ±m √ñrnekleri:**

- Netflix Hystrix(eski) -> circuit breaker implementasyonu √ºnl√ºd√ºr.
- Envoy ve Istio gibi servis mesh' ler circuit breaker, retries ve timeouts saƒülar.
- API Gateway (NGINX, Kong, AWS API GW)

### √ñrnek Senaryo: (√ñdeme Servisi)

1. Kullanƒ±cƒ± √∂deme yapar. -> call PaymentService()
2. Eƒüer PaymentService timeout veya hata verirse: 
- Retry(Maks 3) exponential backoff ile.
- Eƒüer CB a√ßƒ±ldƒ±ysa: fallback-> "√ñdeme i≈üleminiz ≈üuan yava≈ü. L√ºtfen faha sonra tekrar deneyin veya destek ile ilti≈üime ge√ßin."
- Background: enqueue event for manual investigation or retry.

Ayrƒ±ca √∂deme i√ßin idempotency-key kullanƒ±lmalƒ±, tekrar √ßaƒürƒ±da √ßifte √∂deme engellenmeli.

**TERƒ∞MLER**

| `Terim` | `A√ßƒ±klama` | `√ñrnek` |
| --- | --- | --- | 
| Latency | Gecikme s√ºresi(bir isteƒüin gidi≈ü-geli≈ü zamanƒ±) | "Whatsapp mesajƒ± ne kadar s√ºrede kar≈üƒ±ya gidiyor?" | 
| Backoff | Hata olduƒüunda tekrar denemeden √∂nce bekleme s√ºresi | Kapƒ± kilitli -> 1 saniye sonra tekrar dene, yine kilitli -> 2 saniye sonra dene |
| Exponential Backoff | Bekleme s√ºresi katlanarak artƒ±yorsa | 1s-> 2s-> 4s-> 8s ≈üeklinde beklemek |  
| Jitter | Backoff s√ºresine rastgelelilik eklemek | 4 saniye beklemesi gerekirken bazen 3.8s veya 4.2 saniye bekletmek | 
| Thundering Herd | Bir√ßok sistem aynƒ± anda saldƒ±rƒ±ya ge√ßerse olu≈üturduƒüu a≈üƒ±rƒ± y√ºk (s√ºr√º psikolojisi problemi) | 100 telefon aynƒ± anda Amazon'a "≈ûimdi Yenile" dediƒüinde sistem d√º≈üer |
| Enqueue | Kuyruƒüa eklemek | "Bir i≈üi sonra yap" diye bir listeye yazmak | 

* 1Ô∏è‚É£ Circuit breaker hangi √º√ß durumda Open olabilir?

- Pe≈ü pe≈üe √ßok fazla hata olursa (√∂rneƒüin 10 denemenin 7‚Äôsi hata)
- Timeout sayƒ±sƒ± artarsa (servis cevap vermiyor)
- Baƒülantƒ± kurulamazsa (DNS / Network arƒ±zasƒ±)

* 2Ô∏è‚É£ Retry stratejisinde jitter neden √∂nemlidir?

√á√ºnk√º t√ºm sistemler aynƒ± anda tekrar denerse b√ºy√ºk √ß√∂k√º≈ü olur. Jitter k√º√ß√ºk rastgele bekleme koyarak "s√ºr√º halinde saldƒ±rƒ±yƒ±" engeller.

* 3Ô∏è‚É£ Bulkhead nedir?

Kaynaklarƒ± b√∂lmeye denir. √ñrneƒüin "√∂deme servisine ayrƒ±lmƒ±≈ü 20 thread, diƒüer API' lere 20 thread" diye b√∂lmek. B√∂ylece √∂deme servisi yava≈ülasa bile diƒüer servisleri bozmaz.

**Circuit Breaker + Retry + Failover mimarisinin sade ve anla≈üƒ±lƒ±r bir Mermaid diyagramƒ±:**

```mermaid

flowchart TD

Client -->|Request| API_Gateway

API_Gateway -->|Calls| Circuit_Breaker

Circuit_Breaker -->|Service Available| Service_A
Circuit_Breaker -->|Service Down| Retry_Logic

Retry_Logic -->|Max Retries Reached| Failover_Service
Retry_Logic -->|Retry Success| Service_A

Service_A -->|Response| Circuit_Breaker
Failover_Service -->|Fallback Response| Circuit_Breaker

Circuit_Breaker -->|Returns| API_Gateway
API_Gateway -->|Final Response| Client

```

**Basit Okuma Mantƒ±ƒüƒ±:**

| `Bile≈üen` | `A√ßƒ±klama` |
| --- | --- |
| API Gateway | T√ºm isteklerin giri≈ü noktasƒ± | 
| Circuit Breaker | Hatalarƒ± takip eder -> Gerekirse servisi "bloklar" | 
| Retry Logic | Yeniden deneme (backoff + jitter) | 
| Failover Service | Ana servis √ß√∂kerse devreye giren yedek | 
| Final Response | M√º≈üteriye "ya ger√ßek cevap ya da gracefull fallback" g√∂sterilir |

---